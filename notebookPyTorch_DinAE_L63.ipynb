{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebookPyTorch_DinAE_L63.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5KeJmuXrVv/UF53+KfZr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rfablet/DinAE/blob/master/notebookPyTorch_DinAE_L63.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEDUuvB92-Sr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "3196163d-4aef-4c80-a3da-d38e6b577305"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import os\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn import decomposition"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVn8gpSf3HA5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "27ae75d7-42e4-439c-ea59-c0a2a4921783"
      },
      "source": [
        "!pip install torchviz\n",
        "import torchviz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekTtRwg63J1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "76fc2baf-b383-4bea-d79b-3d37a98b3f20"
      },
      "source": [
        "#!mkdir /content/PythonCode\n",
        "#os.mkdir('/content/PythonCode')\n",
        "#os.chdir('/content/PythonCode/')\n",
        "#!git clone https://github.com/CIA-Oceanix/DinAE.git\n",
        "os.chdir('/content/PythonCode/DinAE')\n",
        "!git pull\n",
        "#import DinAE"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/CIA-Oceanix/DinAE\n",
            "   5f13945..a7951d0  master     -> origin/master\n",
            "Updating 5f13945..a7951d0\n",
            "Fast-forward\n",
            " dinAE_solver_torch.py | 20 \u001b[32m++++++++++++++++++++\u001b[m\n",
            " 1 file changed, 20 insertions(+)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mm4jvvi3MiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.mkdir('/content/PythonCode')\n",
        "#os.chdir('/content/PythonCode')\n",
        "#!git clone https://github.com/CIA-Oceanix/DinAE.git\n",
        "os.chdir('/content/PythonCode/DinAE')\n",
        "import dinAE_solver_torch as dinAE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Hk-uDe3Rl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "d91c8cd8-9bb8-47aa-a333-614702f64e3a"
      },
      "source": [
        "# mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls /content/drive/My\\ Drive/ResearchData/patchDataset_OcciputData.nc\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/')\n",
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'/content/drive/My Drive/ResearchData/patchDataset_OcciputData.nc'\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1FjHV_K3W9s",
        "colab_type": "text"
      },
      "source": [
        "# L63 Data simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHcF-wxR3VEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/Colab Notebooks/AnDA')\n",
        "from AnDA_codes.AnDA_generate_data import AnDA_generate_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS-bV-BE3g90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0cc9cdf1-8746-452a-f372-160a033d98e1"
      },
      "source": [
        "from scipy.integrate import odeint\n",
        "from scipy.integrate import solve_ivp\n",
        "from AnDA_codes.AnDA_dynamical_models import AnDA_Lorenz_63, AnDA_Lorenz_96\n",
        "\n",
        "class GD:\n",
        "    model = 'Lorenz_63'\n",
        "    class parameters:\n",
        "        sigma = 10.0\n",
        "        rho = 28.0\n",
        "        beta = 8.0/3\n",
        "    dt_integration = 0.01 # integration time\n",
        "    dt_states = 1 # number of integeration times between consecutive states (for xt and catalog)\n",
        "    dt_obs = 8 # number of integration times between consecutive observations (for yo)\n",
        "    var_obs = np.array([0,1,2]) # indices of the observed variables\n",
        "    nb_loop_train = 10**2 # size of the catalog\n",
        "    nb_loop_test = 10000 # size of the true state and noisy observations\n",
        "    sigma2_catalog = 0.0 # variance of the model error to generate the catalog\n",
        "    sigma2_obs = 2.0 # variance of the observation error to generate observation\n",
        "\n",
        "GD = GD()    \n",
        "y0 = np.array([8.0,0.0,30.0])\n",
        "tt = np.arange(GD.dt_integration,GD.nb_loop_test*GD.dt_integration+0.000001,GD.dt_integration)\n",
        "#S = odeint(AnDA_Lorenz_63,x0,np.arange(0,5+0.000001,GD.dt_integration),args=(GD.parameters.sigma,GD.parameters.rho,GD.parameters.beta));\n",
        "S = solve_ivp(fun=lambda t,y: AnDA_Lorenz_63(y,t,GD.parameters.sigma,GD.parameters.rho,GD.parameters.beta),t_span=[0.,5+0.000001],y0=y0,first_step=GD.dt_integration,t_eval=np.arange(0,5+0.000001,GD.dt_integration),method='RK45')\n",
        "\n",
        "y0 = S.y[:,-1];\n",
        "S = solve_ivp(fun=lambda t,y: AnDA_Lorenz_63(y,t,GD.parameters.sigma,GD.parameters.rho,GD.parameters.beta),t_span=[0.01,GD.nb_loop_test+0.000001],y0=y0,first_step=GD.dt_integration,t_eval=tt,method='RK45')\n",
        "S = S.y.transpose()\n",
        "\n",
        "print(S.shape)\n",
        "plt.figure(1)\n",
        "for jj in range(0,3):\n",
        "  plt.subplot(131+jj)\n",
        "  plt.plot(S[0:500,jj])\n",
        "  \n",
        "class time_series:\n",
        "  values = 0.\n",
        "  time   = 0.\n",
        "  \n",
        "xt = time_series()\n",
        "xt.values = S\n",
        "xt.time   = tt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEiBfQ+HEeV42NebvgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDhSP2oL5wrP",
        "colab_type": "text"
      },
      "source": [
        "# L63 Patch data extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wAnHDEw3pg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction import image\n",
        "\n",
        "time_step = 2\n",
        "\n",
        "# extract subsequences\n",
        "dT = 200\n",
        "dataTrainingNoNaN = image.extract_patches_2d(xt.values[0:7000:time_step,:],(dT,3),50000)\n",
        "dataTestNoNaN     = image.extract_patches_2d(xt.values[7000::time_step,:],(dT,3),10000)\n",
        "\n",
        "# create missing data\n",
        "rateMissingData = 0.75\n",
        "indRand         = np.random.permutation(dataTrainingNoNaN.shape[0]*dataTrainingNoNaN.shape[1]*dataTrainingNoNaN.shape[2])\n",
        "indRand         = indRand[0:int(rateMissingData*len(indRand))]\n",
        "dataTraining    = np.copy(dataTrainingNoNaN).reshape((dataTrainingNoNaN.shape[0]*dataTrainingNoNaN.shape[1]*dataTrainingNoNaN.shape[2],1))\n",
        "dataTraining[indRand] = float('nan')\n",
        "dataTraining    = np.reshape(dataTraining,(dataTrainingNoNaN.shape[0],dataTrainingNoNaN.shape[1],dataTrainingNoNaN.shape[2]))\n",
        "\n",
        "indRand         = np.random.permutation(dataTestNoNaN.shape[0]*dataTestNoNaN.shape[1]*dataTestNoNaN.shape[2])\n",
        "indRand         = indRand[0:int(rateMissingData*len(indRand))]\n",
        "dataTest        = np.copy(dataTestNoNaN).reshape((dataTestNoNaN.shape[0]*dataTestNoNaN.shape[1]*dataTestNoNaN.shape[2],1))\n",
        "dataTest[indRand] = float('nan')\n",
        "dataTest          = np.reshape(dataTest,(dataTestNoNaN.shape[0],dataTestNoNaN.shape[1],dataTestNoNaN.shape[2]))\n",
        "\n",
        "# data normalization\n",
        "if 1*1:\n",
        "  if 1*1:\n",
        "    meanTr       = np.nanmean(dataTraining[:]) \n",
        "    dataTraining = dataTraining - meanTr\n",
        "    dataTest     = dataTest - meanTr\n",
        "\n",
        "    # scale wrt std\n",
        "    stdTr        = np.sqrt( np.nanmean( dataTraining**2 ) )\n",
        "    dataTraining = dataTraining / stdTr\n",
        "    dataTest     = dataTest / stdTr\n",
        "\n",
        "    dataTrainingNoNaN = (dataTrainingNoNaN - meanTr ) / stdTr\n",
        "    dataTestNoNaN     = (dataTestNoNaN - meanTr ) / stdTr\n",
        "  else:\n",
        "    mini = np.amin(dataTraining[:])\n",
        "    maxi = np.amax(dataTraining[:])\n",
        "\n",
        "    dataTraining = (dataTraining - mini ) /(maxi-mini)\n",
        "    dataTest     = (dataTest - mini ) /(maxi-mini)\n",
        "\n",
        "    dataTrainingNoNaN = (dataTrainingNoNaN - mini ) /(maxi-mini)\n",
        "    dataTestNoNaN     = (dataTestNoNaN - mini ) /(maxi-mini)\n",
        "\n",
        "# set to NaN patch boundaries\n",
        "dataTraining[:,0:10,:] =  float('nan')\n",
        "dataTest[:,0:10,:]     =  float('nan')\n",
        "dataTraining[:,dT-10:dT,:] =  float('nan')\n",
        "dataTest[:,dT-10:dT,:]     =  float('nan')\n",
        "\n",
        "# mask for NaN\n",
        "maskTraining = (dataTraining == dataTraining).astype('float')\n",
        "maskTest     = ( dataTest    ==  dataTest   ).astype('float')\n",
        "\n",
        "dataTraining = np.nan_to_num(dataTraining)\n",
        "dataTest     = np.nan_to_num(dataTest)\n",
        "\n",
        "# set to NaN patch boundaries\n",
        "#dataTraining[:,0:5,:] =  dataTrainingNoNaN[:,0:5,:]\n",
        "#dataTest[:,0:5,:]     =  dataTestNoNaN[:,0:5,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgE16y2Q8pm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train         = np.moveaxis( dataTrainingNoNaN.reshape((dataTrainingNoNaN.shape[0],dataTrainingNoNaN.shape[1],dataTrainingNoNaN.shape[2])) , 2 , 1 )\n",
        "x_train_missing = np.moveaxis( dataTraining.reshape((dataTraining.shape[0],dataTraining.shape[1],dataTraining.shape[2])) , 2 , 1 )\n",
        "mask_train      = np.moveaxis( maskTraining.reshape((maskTraining.shape[0],maskTraining.shape[1],maskTraining.shape[2])) , 2 , 1 )\n",
        "\n",
        "x_test         = np.moveaxis( dataTestNoNaN.reshape((dataTestNoNaN.shape[0],dataTestNoNaN.shape[1],dataTestNoNaN.shape[2])) , 2 , 1 )\n",
        "x_test_missing = np.moveaxis( dataTest.reshape((dataTest.shape[0],dataTest.shape[1],dataTest.shape[2])) , 2 , 1 )\n",
        "mask_test      = np.moveaxis( maskTest.reshape((maskTest.shape[0],maskTest.shape[1],maskTest.shape[2])) , 2 , 1 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjc6TeeJ6B5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "06341909-5252-4918-dd5a-91aad6540cbc"
      },
      "source": [
        "idx = 180\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(1)\n",
        "for jj in range(0,3):\n",
        "  indjj = 131+jj\n",
        "  plt.subplot(indjj)\n",
        "  plt.plot(x_train_missing[idx,jj,:],'k.')\n",
        "  plt.plot(x_train[idx,jj,:],'b-')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMoxRI6AoijKMUSOgKIoyjPl/PTaNNBOKRlAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87n8ap6N8E8p",
        "colab_type": "text"
      },
      "source": [
        "# PCA Decomposition & AE architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR-wA2cV8EVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DimAE      = 20#50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YIGzl5D8M-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ce484a1d-bc0c-4d08-fdfd-7d087a03f0b7"
      },
      "source": [
        "# PCA decomposition\n",
        "pca              = decomposition.PCA(DimAE)\n",
        "pca.fit(np.reshape(x_train,(x_train.shape[0],x_train.shape[1]*x_train.shape[2])))\n",
        "\n",
        "rec_PCA_Tt       = pca.transform(np.reshape(x_test,(x_test.shape[0],x_test.shape[1]*x_test.shape[2])))\n",
        "rec_PCA_Tt[:,DimAE:] = 0.\n",
        "rec_PCA_Tt       = pca.inverse_transform(rec_PCA_Tt)\n",
        "mse_PCA_Tt       = np.mean( (rec_PCA_Tt - x_test.reshape((x_test.shape[0],x_test.shape[1]*x_test.shape[2])))**2 )\n",
        "var_Tt           = np.mean( (x_test-np.mean(x_train,axis=0))** 2 )\n",
        "exp_var_PCA_Tt   = 1. - mse_PCA_Tt / var_Tt\n",
        "\n",
        "print(\".......... PCA Dim = %d\"%(DimAE))\n",
        "print('.... explained variance PCA (Tr) : %.2f%%'%(100.*np.cumsum(pca.explained_variance_ratio_)[DimAE-1]))\n",
        "print('.... explained variance PCA (Tt) : %.2f%%'%(100.*exp_var_PCA_Tt))\n",
        "\n",
        "# visualize PCs and associated projection\n",
        "PC              = np.zeros((DimAE+1,x_test.shape[1]*x_test.shape[2])) * float('NaN')                        \n",
        "PC[1:DimAE+1,:] = pca.components_\n",
        "PC[0,:]         = pca.mean_\n",
        "PC              = np.reshape(PC,(DimAE+1,x_test.shape[1],x_test.shape[2]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".......... PCA Dim = 20\n",
            ".... explained variance PCA (Tr) : 84.14%\n",
            ".... explained variance PCA (Tt) : 82.23%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h3XytUGFfOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConstrainedConv1d(torch.nn.Conv1d):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 dilation=1,\n",
        "                 groups=1,\n",
        "                 bias=True):\n",
        "        super(ConstrainedConv1d,\n",
        "              self).__init__(in_channels, out_channels, kernel_size, stride,\n",
        "                             padding, dilation, groups, bias)\n",
        "        with torch.no_grad():\n",
        "          self.weight[:,:,int(self.weight.size(2)/2)+1] = 0.0\n",
        "    def forward(self, input):\n",
        "        return torch.nn.functional.conv1d(input, self.weight, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glhN9hqk-fsa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "87d3939e-5a2c-4027-9ae9-8ca6f497fa1a"
      },
      "source": [
        "flagAEType = 2\n",
        "DimAE      = 10#50\n",
        "dropout    = 0.05\n",
        "wl2        = 0\n",
        "shapeData = x_train.shape[1:]\n",
        "\n",
        "if flagAEType == 0: ## MLP-AE\n",
        "\n",
        "  class Encoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Encoder, self).__init__()\n",
        "          self.fc1 = torch.nn.Linear(shapeData[0]*shapeData[1],6*DimAE)\n",
        "          self.fc2 = torch.nn.Linear(6*DimAE,2*DimAE)\n",
        "          self.fc3 = torch.nn.Linear(2*DimAE,DimAE)\n",
        "\n",
        "      def forward(self, x):\n",
        "          #x = self.fc1( torch.nn.Flatten(x) )\n",
        "          x = self.fc1( x.view(-1,shapeData[0]*shapeData[1]) )\n",
        "          x = self.fc2( F.relu(x) )\n",
        "          x = self.fc3( F.relu(x) )\n",
        "          return x\n",
        "\n",
        "  encoder = Encoder()\n",
        "\n",
        "  class Decoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Decoder, self).__init__()\n",
        "          self.fc1 = torch.nn.Linear(DimAE,10*DimAE)\n",
        "          self.fc2 = torch.nn.Linear(10*DimAE,20*DimAE)\n",
        "          self.fc3 = torch.nn.Linear(20*DimAE,shapeData[0]*shapeData[1])\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.fc1( x )\n",
        "          x = self.fc2( F.relu(x) )\n",
        "          x = self.fc3( F.relu(x) )\n",
        "          x = x.view(-1,shapeData[0],shapeData[1])\n",
        "          return x\n",
        "\n",
        "elif flagAEType == 1: ## Conv-AE\n",
        "  Wpool_i = np.floor(  (np.floor((x_train.shape[2]-2)/2)-2)/2 ).astype(int) \n",
        "\n",
        "  class Encoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Encoder, self).__init__()\n",
        "          self.conv1 = torch.nn.Conv1d(shapeData[0],DimAE,3,padding=0)\n",
        "          self.pool1 = torch.nn.AvgPool1d(2)\n",
        "          self.conv2 = torch.nn.Conv2d(DimAE,2*DimAE,3,padding=0)\n",
        "          self.pool2 = torch.nn.AvgPool1d(2)\n",
        "          self.conv3 = torch.nn.Conv2d(2*DimAE,4*DimAE,Wpool_i,padding=0)\n",
        "          self.conv4 = torch.nn.Conv2d(4*DimAE,DimAE,(1,1),padding=0)\n",
        "\n",
        "      def forward(self, x):\n",
        "          #x = self.fc1( torch.nn.Flatten(x) )\n",
        "          x = self.conv1( x )\n",
        "          x = self.pool1(x)\n",
        "          x = self.conv2( F.relu(x) )\n",
        "          x = self.pool2(x)\n",
        "          x = self.conv3( F.relu(x) )\n",
        "          x = self.conv4( F.relu(x) )\n",
        "          x = x.view(-1,DimAE)\n",
        "          return x\n",
        "\n",
        "  class Decoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Decoder, self).__init__()\n",
        "          #self.conv1Tr = torch.nn.ConvTranspose2d(DimAE,1,(x_train.shape[1],x_train.shape[2]),stride=(x_train.shape[1],x_train.shape[2]),bias=False)\n",
        "          self.conv1Tr = torch.nn.ConvTranspose1d(DimAE,DimAE,int(x_train.shape[2]/2),stride=int(x_train.shape[2]/2),bias=False)\n",
        "          self.conv11   = torch.nn.Conv1d(DimAE,DimAE,3,padding=1)\n",
        "          self.conv12   = torch.nn.Conv1d(DimAE,DimAE,3,padding=1)\n",
        "          self.conv2Tr = torch.nn.ConvTranspose1d(DimAE,DimAE,2,stride=2,bias=False)\n",
        "          #self.resnet  = self._make_ResNet(2,DimAE,5,3,1)\n",
        "          self.resnet = dinAE.ResNetConv1D(2,DimAE,5,3,1)\n",
        "          self.convF   = torch.nn.Conv1d(DimAE,1,(1,1),padding=0)\n",
        "      def _make_ResNet(self,Nblocks,dim,K,kernel_size, padding):\n",
        "          layers = []\n",
        "          for kk in range(0,Nblocks):\n",
        "            layers.append(torch.nn.Conv2d(dim,K*dim,kernel_size,padding=padding,bias=False))\n",
        "            layers.append(torch.nn.Conv2d(K*dim,dim,kernel_size,padding=padding,bias=False))\n",
        "\n",
        "          return torch.nn.Sequential(*layers)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = x.view(-1,DimAE,1,1)\n",
        "          x = self.conv1Tr( x )\n",
        "          x = torch.add(self.conv12( F.relu( self.conv11(x) ) ),x)\n",
        "          x = torch.add(self.conv12( F.relu( self.conv11(x) ) ),x)\n",
        "          x = self.conv2Tr( x )\n",
        "          x = self.resnet(x)\n",
        "          x = self.convF(x)\n",
        "\n",
        "          #x = torch.add(self.conv22( F.relu( self.conv21(x) ) ),x)\n",
        "          #x = torch.add(self.conv22( F.relu( self.conv21(x) ) ),x)\n",
        "          #x = self.conv3( x )\n",
        "          #x = x.view(-1,shapeData[0],shapeData[1],shapeData[2])\n",
        "          return x\n",
        "\n",
        "elif flagAEType == 2: ## Conv model with no use of the central point\n",
        "  class Encoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Encoder, self).__init__()\n",
        "          self.pool1 = torch.nn.AvgPool1d(2)\n",
        "          self.conv1 = dinAE.ConstrainedConv1d(shapeData[0],shapeData[0]*DimAE,7,padding=3)\n",
        "          self.conv2 = torch.nn.Conv1d(shapeData[0]*DimAE,2*shapeData[0]*DimAE,1,padding=0)\n",
        "          self.conv3 = torch.nn.Conv1d(2*shapeData[0]*DimAE,4*shapeData[0]*DimAE,1,padding=0)\n",
        "          self.conv4 = torch.nn.Conv1d(4*shapeData[0]*DimAE,8*shapeData[0]*DimAE,1,padding=0)\n",
        "\n",
        "          self.conv2Tr = torch.nn.ConvTranspose1d(8*shapeData[0]*DimAE,8*shapeData[0]*DimAE,2,stride=2,bias=False)          \n",
        "          self.conv5 = torch.nn.Conv1d(8*shapeData[0]*DimAE,16*shapeData[0]*DimAE,3,padding=1)\n",
        "          self.conv6 = torch.nn.Conv1d(16*shapeData[0]*DimAE,shapeData[0],3,padding=1)\n",
        "\n",
        "      def forward(self, x):\n",
        "          #x = self.fc1( torch.nn.Flatten(x) )\n",
        "          x = self.pool1( x )\n",
        "          x = self.conv1(x)\n",
        "          x = self.conv2( F.relu(x) )\n",
        "          x = self.conv3( F.relu(x) )\n",
        "          x = self.conv4( F.relu(x) )\n",
        "          x = self.conv2Tr( x )\n",
        "          x = self.conv5( x )\n",
        "          x = self.conv6( x )\n",
        "          x = x.view(-1,shapeData[0],shapeData[1])\n",
        "          return x\n",
        "\n",
        "  class Decoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Decoder, self).__init__()\n",
        "\n",
        "      def forward(self, x):\n",
        "          return torch.mul(1.,x)\n",
        "\n",
        "class Model_AE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model_AE, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder( x )\n",
        "        x = self.decoder( x )\n",
        "        return x\n",
        "\n",
        "model_AE = Model_AE()\n",
        "\n",
        "print(model_AE)\n",
        "print('Number of trainable parameters = %d'%(sum(p.numel() for p in model_AE.parameters() if p.requires_grad)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model_AE(\n",
            "  (encoder): Encoder(\n",
            "    (pool1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
            "    (conv1): ConstrainedConv1d(3, 30, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "    (conv2): Conv1d(30, 60, kernel_size=(1,), stride=(1,))\n",
            "    (conv3): Conv1d(60, 120, kernel_size=(1,), stride=(1,))\n",
            "    (conv4): Conv1d(120, 240, kernel_size=(1,), stride=(1,))\n",
            "    (conv2Tr): ConvTranspose1d(240, 240, kernel_size=(2,), stride=(2,), bias=False)\n",
            "    (conv5): Conv1d(240, 480, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (conv6): Conv1d(480, 3, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  )\n",
            "  (decoder): Decoder()\n",
            ")\n",
            "Number of trainable parameters = 504483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEnga19oCNFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fb1ff1d-19b1-4019-f441-31d43b1e9fa6"
      },
      "source": [
        "#Model visualisation\n",
        "inputs = torch.randn(21,3,200)\n",
        "y = model_AE(torch.autograd.Variable(inputs))\n",
        "print(y.size())\n",
        "torchviz.make_dot(y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([21, 3, 200])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fec86d38198>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"589pt\" height=\"961pt\"\n viewBox=\"0.00 0.00 589.20 961.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.5676 .5676) rotate(0) translate(4 1689)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1689 1034,-1689 1034,4 -4,4\"/>\n<!-- 140653847496464 -->\n<g id=\"node1\" class=\"node\">\n<title>140653847496464</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"941.5,-21 850.5,-21 850.5,0 941.5,0 941.5,-21\"/>\n<text text-anchor=\"middle\" x=\"896\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140653847495960 -->\n<g id=\"node2\" class=\"node\">\n<title>140653847495960</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"941.5,-78 850.5,-78 850.5,-57 941.5,-57 941.5,-78\"/>\n<text text-anchor=\"middle\" x=\"896\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 140653847495960&#45;&gt;140653847496464 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140653847495960&#45;&gt;140653847496464</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M896,-56.7787C896,-49.6134 896,-39.9517 896,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"899.5001,-31.1732 896,-21.1732 892.5001,-31.1732 899.5001,-31.1732\"/>\n</g>\n<!-- 140653847494896 -->\n<g id=\"node3\" class=\"node\">\n<title>140653847494896</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"951.5,-135 840.5,-135 840.5,-114 951.5,-114 951.5,-135\"/>\n<text text-anchor=\"middle\" x=\"896\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SqueezeBackward1</text>\n</g>\n<!-- 140653847494896&#45;&gt;140653847495960 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140653847494896&#45;&gt;140653847495960</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M896,-113.7787C896,-106.6134 896,-96.9517 896,-88.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"899.5001,-88.1732 896,-78.1732 892.5001,-88.1732 899.5001,-88.1732\"/>\n</g>\n<!-- 140653847495008 -->\n<g id=\"node4\" class=\"node\">\n<title>140653847495008</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"977,-192 815,-192 815,-171 977,-171 977,-192\"/>\n<text text-anchor=\"middle\" x=\"896\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140653847495008&#45;&gt;140653847494896 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140653847495008&#45;&gt;140653847494896</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M896,-170.7787C896,-163.6134 896,-153.9517 896,-145.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"899.5001,-145.1732 896,-135.1732 892.5001,-145.1732 899.5001,-145.1732\"/>\n</g>\n<!-- 140653847498256 -->\n<g id=\"node5\" class=\"node\">\n<title>140653847498256</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"816,-256 692,-256 692,-235 816,-235 816,-256\"/>\n<text text-anchor=\"middle\" x=\"754\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653847498256&#45;&gt;140653847495008 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140653847498256&#45;&gt;140653847495008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M777.4484,-234.9317C800.881,-224.3705 837.1028,-208.0452 863.3431,-196.2186\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"864.8381,-199.3839 872.5168,-192.084 861.9618,-193.0022 864.8381,-199.3839\"/>\n</g>\n<!-- 140653847498536 -->\n<g id=\"node6\" class=\"node\">\n<title>140653847498536</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"809.5,-327 698.5,-327 698.5,-306 809.5,-306 809.5,-327\"/>\n<text text-anchor=\"middle\" x=\"754\" y=\"-313.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SqueezeBackward1</text>\n</g>\n<!-- 140653847498536&#45;&gt;140653847498256 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140653847498536&#45;&gt;140653847498256</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M754,-305.7166C754,-295.3953 754,-279.5401 754,-266.6896\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"757.5001,-266.3848 754,-256.3849 750.5001,-266.3849 757.5001,-266.3848\"/>\n</g>\n<!-- 140653847498648 -->\n<g id=\"node7\" class=\"node\">\n<title>140653847498648</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"835,-391 673,-391 673,-370 835,-370 835,-391\"/>\n<text text-anchor=\"middle\" x=\"754\" y=\"-377.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140653847498648&#45;&gt;140653847498536 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140653847498648&#45;&gt;140653847498536</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M754,-369.9317C754,-361.0913 754,-348.2122 754,-337.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"757.5001,-337.2979 754,-327.2979 750.5001,-337.2979 757.5001,-337.2979\"/>\n</g>\n<!-- 140653847498424 -->\n<g id=\"node8\" class=\"node\">\n<title>140653847498424</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"674,-455 550,-455 550,-434 674,-434 674,-455\"/>\n<text text-anchor=\"middle\" x=\"612\" y=\"-441.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653847498424&#45;&gt;140653847498648 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140653847498424&#45;&gt;140653847498648</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M635.4484,-433.9317C658.881,-423.3705 695.1028,-407.0452 721.3431,-395.2186\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"722.8381,-398.3839 730.5168,-391.084 719.9618,-392.0022 722.8381,-398.3839\"/>\n</g>\n<!-- 140653847497472 -->\n<g id=\"node9\" class=\"node\">\n<title>140653847497472</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"667.5,-526 556.5,-526 556.5,-505 667.5,-505 667.5,-526\"/>\n<text text-anchor=\"middle\" x=\"612\" y=\"-512.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SqueezeBackward1</text>\n</g>\n<!-- 140653847497472&#45;&gt;140653847498424 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140653847497472&#45;&gt;140653847498424</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M612,-504.7166C612,-494.3953 612,-478.5401 612,-465.6896\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"615.5001,-465.3848 612,-455.3849 608.5001,-465.3849 615.5001,-465.3848\"/>\n</g>\n<!-- 140653847497696 -->\n<g id=\"node10\" class=\"node\">\n<title>140653847497696</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"701.5,-590 522.5,-590 522.5,-569 701.5,-569 701.5,-590\"/>\n<text text-anchor=\"middle\" x=\"612\" y=\"-576.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SlowConvTranspose2DBackward</text>\n</g>\n<!-- 140653847497696&#45;&gt;140653847497472 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140653847497696&#45;&gt;140653847497472</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M612,-568.9317C612,-560.0913 612,-547.2122 612,-536.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"615.5001,-536.2979 612,-526.2979 608.5001,-536.2979 615.5001,-536.2979\"/>\n</g>\n<!-- 140653851140504 -->\n<g id=\"node11\" class=\"node\">\n<title>140653851140504</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"603,-647 479,-647 479,-626 603,-626 603,-647\"/>\n<text text-anchor=\"middle\" x=\"541\" y=\"-633.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653851140504&#45;&gt;140653847497696 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140653851140504&#45;&gt;140653847497696</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M554.3546,-625.7787C564.569,-617.5784 578.8561,-606.1085 590.6511,-596.6393\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"593.0985,-599.1629 598.7053,-590.1732 588.7162,-593.7043 593.0985,-599.1629\"/>\n</g>\n<!-- 140653847698456 -->\n<g id=\"node12\" class=\"node\">\n<title>140653847698456</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"596.5,-711 485.5,-711 485.5,-690 596.5,-690 596.5,-711\"/>\n<text text-anchor=\"middle\" x=\"541\" y=\"-697.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SqueezeBackward1</text>\n</g>\n<!-- 140653847698456&#45;&gt;140653851140504 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140653847698456&#45;&gt;140653851140504</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M541,-689.9317C541,-681.0913 541,-668.2122 541,-657.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"544.5001,-657.2979 541,-647.2979 537.5001,-657.2979 544.5001,-657.2979\"/>\n</g>\n<!-- 140653847698008 -->\n<g id=\"node13\" class=\"node\">\n<title>140653847698008</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"622,-775 460,-775 460,-754 622,-754 622,-775\"/>\n<text text-anchor=\"middle\" x=\"541\" y=\"-761.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140653847698008&#45;&gt;140653847698456 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140653847698008&#45;&gt;140653847698456</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M541,-753.9317C541,-745.0913 541,-732.2122 541,-721.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"544.5001,-721.2979 541,-711.2979 537.5001,-721.2979 544.5001,-721.2979\"/>\n</g>\n<!-- 140653848071752 -->\n<g id=\"node14\" class=\"node\">\n<title>140653848071752</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"461,-839 337,-839 337,-818 461,-818 461,-839\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-825.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653848071752&#45;&gt;140653847698008 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140653848071752&#45;&gt;140653847698008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M422.4484,-817.9317C445.881,-807.3705 482.1028,-791.0452 508.3431,-779.2186\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"509.8381,-782.3839 517.5168,-775.084 506.9618,-776.0022 509.8381,-782.3839\"/>\n</g>\n<!-- 140653848071360 -->\n<g id=\"node15\" class=\"node\">\n<title>140653848071360</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"446,-910 352,-910 352,-889 446,-889 446,-910\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-896.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140653848071360&#45;&gt;140653848071752 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140653848071360&#45;&gt;140653848071752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M399,-888.7166C399,-878.3953 399,-862.5401 399,-849.6896\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"402.5001,-849.3848 399,-839.3849 395.5001,-849.3849 402.5001,-849.3848\"/>\n</g>\n<!-- 140653848071864 -->\n<g id=\"node16\" class=\"node\">\n<title>140653848071864</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"454.5,-974 343.5,-974 343.5,-953 454.5,-953 454.5,-974\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-960.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SqueezeBackward1</text>\n</g>\n<!-- 140653848071864&#45;&gt;140653848071360 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140653848071864&#45;&gt;140653848071360</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M399,-952.9317C399,-944.0913 399,-931.2122 399,-920.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"402.5001,-920.2979 399,-910.2979 395.5001,-920.2979 402.5001,-920.2979\"/>\n</g>\n<!-- 140653848071920 -->\n<g id=\"node17\" class=\"node\">\n<title>140653848071920</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"480,-1031 318,-1031 318,-1010 480,-1010 480,-1031\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-1017.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140653848071920&#45;&gt;140653848071864 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140653848071920&#45;&gt;140653848071864</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M399,-1009.7787C399,-1002.6134 399,-992.9517 399,-984.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"402.5001,-984.1732 399,-974.1732 395.5001,-984.1732 402.5001,-984.1732\"/>\n</g>\n<!-- 140653848071528 -->\n<g id=\"node18\" class=\"node\">\n<title>140653848071528</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"319,-1095 195,-1095 195,-1074 319,-1074 319,-1095\"/>\n<text text-anchor=\"middle\" x=\"257\" y=\"-1081.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653848071528&#45;&gt;140653848071920 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140653848071528&#45;&gt;140653848071920</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M280.4484,-1073.9317C303.881,-1063.3705 340.1028,-1047.0452 366.3431,-1035.2186\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"367.8381,-1038.3839 375.5168,-1031.084 364.9618,-1032.0022 367.8381,-1038.3839\"/>\n</g>\n<!-- 140653850888792 -->\n<g id=\"node19\" class=\"node\">\n<title>140653850888792</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"304,-1166 210,-1166 210,-1145 304,-1145 304,-1166\"/>\n<text text-anchor=\"middle\" x=\"257\" y=\"-1152.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140653850888792&#45;&gt;140653848071528 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140653850888792&#45;&gt;140653848071528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M257,-1144.7166C257,-1134.3953 257,-1118.5401 257,-1105.6896\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.5001,-1105.3848 257,-1095.3849 253.5001,-1105.3849 260.5001,-1105.3848\"/>\n</g>\n<!-- 140653850888904 -->\n<g id=\"node20\" class=\"node\">\n<title>140653850888904</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"312.5,-1230 201.5,-1230 201.5,-1209 312.5,-1209 312.5,-1230\"/>\n<text text-anchor=\"middle\" x=\"257\" y=\"-1216.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SqueezeBackward1</text>\n</g>\n<!-- 140653850888904&#45;&gt;140653850888792 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140653850888904&#45;&gt;140653850888792</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M257,-1208.9317C257,-1200.0913 257,-1187.2122 257,-1176.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.5001,-1176.2979 257,-1166.2979 253.5001,-1176.2979 260.5001,-1176.2979\"/>\n</g>\n<!-- 140653850889240 -->\n<g id=\"node21\" class=\"node\">\n<title>140653850889240</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"338,-1287 176,-1287 176,-1266 338,-1266 338,-1287\"/>\n<text text-anchor=\"middle\" x=\"257\" y=\"-1273.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140653850889240&#45;&gt;140653850888904 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140653850889240&#45;&gt;140653850888904</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M257,-1265.7787C257,-1258.6134 257,-1248.9517 257,-1240.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.5001,-1240.1732 257,-1230.1732 253.5001,-1240.1732 260.5001,-1240.1732\"/>\n</g>\n<!-- 140653847501680 -->\n<g id=\"node22\" class=\"node\">\n<title>140653847501680</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"177,-1351 53,-1351 53,-1330 177,-1330 177,-1351\"/>\n<text text-anchor=\"middle\" x=\"115\" y=\"-1337.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653847501680&#45;&gt;140653850889240 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140653847501680&#45;&gt;140653850889240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M138.4484,-1329.9317C161.881,-1319.3705 198.1028,-1303.0452 224.3431,-1291.2186\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"225.8381,-1294.3839 233.5168,-1287.084 222.9618,-1288.0022 225.8381,-1294.3839\"/>\n</g>\n<!-- 140653847500616 -->\n<g id=\"node23\" class=\"node\">\n<title>140653847500616</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"162,-1422 68,-1422 68,-1401 162,-1401 162,-1422\"/>\n<text text-anchor=\"middle\" x=\"115\" y=\"-1408.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 140653847500616&#45;&gt;140653847501680 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140653847500616&#45;&gt;140653847501680</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M115,-1400.7166C115,-1390.3953 115,-1374.5401 115,-1361.6896\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.5001,-1361.3848 115,-1351.3849 111.5001,-1361.3849 118.5001,-1361.3848\"/>\n</g>\n<!-- 140653847502408 -->\n<g id=\"node24\" class=\"node\">\n<title>140653847502408</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"170.5,-1486 59.5,-1486 59.5,-1465 170.5,-1465 170.5,-1486\"/>\n<text text-anchor=\"middle\" x=\"115\" y=\"-1472.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SqueezeBackward1</text>\n</g>\n<!-- 140653847502408&#45;&gt;140653847500616 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140653847502408&#45;&gt;140653847500616</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M115,-1464.9317C115,-1456.0913 115,-1443.2122 115,-1432.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.5001,-1432.2979 115,-1422.2979 111.5001,-1432.2979 118.5001,-1432.2979\"/>\n</g>\n<!-- 140653847500840 -->\n<g id=\"node25\" class=\"node\">\n<title>140653847500840</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"196,-1543 34,-1543 34,-1522 196,-1522 196,-1543\"/>\n<text text-anchor=\"middle\" x=\"115\" y=\"-1529.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140653847500840&#45;&gt;140653847502408 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140653847500840&#45;&gt;140653847502408</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M115,-1521.7787C115,-1514.6134 115,-1504.9517 115,-1496.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.5001,-1496.1732 115,-1486.1732 111.5001,-1496.1732 118.5001,-1496.1732\"/>\n</g>\n<!-- 140653847501400 -->\n<g id=\"node26\" class=\"node\">\n<title>140653847501400</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"124,-1607 0,-1607 0,-1586 124,-1586 124,-1607\"/>\n<text text-anchor=\"middle\" x=\"62\" y=\"-1593.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653847501400&#45;&gt;140653847500840 -->\n<g id=\"edge25\" class=\"edge\">\n<title>140653847501400&#45;&gt;140653847500840</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M70.7519,-1585.9317C78.5304,-1576.5388 90.0846,-1562.5865 99.4356,-1551.2947\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"102.3755,-1553.2322 106.058,-1543.2979 96.9841,-1548.7675 102.3755,-1553.2322\"/>\n</g>\n<!-- 140653847500168 -->\n<g id=\"node27\" class=\"node\">\n<title>140653847500168</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"93.5,-1685 30.5,-1685 30.5,-1650 93.5,-1650 93.5,-1685\"/>\n<text text-anchor=\"middle\" x=\"62\" y=\"-1657.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (30, 3, 7)</text>\n</g>\n<!-- 140653847500168&#45;&gt;140653847501400 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140653847500168&#45;&gt;140653847501400</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M62,-1649.9494C62,-1640.058 62,-1627.6435 62,-1617.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"65.5001,-1617.0288 62,-1607.0288 58.5001,-1617.0289 65.5001,-1617.0288\"/>\n</g>\n<!-- 140653847501008 -->\n<g id=\"node28\" class=\"node\">\n<title>140653847501008</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"196,-1614 142,-1614 142,-1579 196,-1579 196,-1614\"/>\n<text text-anchor=\"middle\" x=\"169\" y=\"-1586.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (30)</text>\n</g>\n<!-- 140653847501008&#45;&gt;140653847500840 -->\n<g id=\"edge27\" class=\"edge\">\n<title>140653847501008&#45;&gt;140653847500840</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M153.958,-1578.6724C146.674,-1570.0396 137.9349,-1559.6821 130.5812,-1550.9666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"133.1717,-1548.6093 124.0479,-1543.2234 127.8216,-1553.1234 133.1717,-1548.6093\"/>\n</g>\n<!-- 140653847500504 -->\n<g id=\"node29\" class=\"node\">\n<title>140653847500504</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"319,-1351 195,-1351 195,-1330 319,-1330 319,-1351\"/>\n<text text-anchor=\"middle\" x=\"257\" y=\"-1337.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653847500504&#45;&gt;140653850889240 -->\n<g id=\"edge28\" class=\"edge\">\n<title>140653847500504&#45;&gt;140653850889240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M257,-1329.9317C257,-1321.0913 257,-1308.2122 257,-1297.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.5001,-1297.2979 257,-1287.2979 253.5001,-1297.2979 260.5001,-1297.2979\"/>\n</g>\n<!-- 140653847502128 -->\n<g id=\"node30\" class=\"node\">\n<title>140653847502128</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"291.5,-1429 222.5,-1429 222.5,-1394 291.5,-1394 291.5,-1429\"/>\n<text text-anchor=\"middle\" x=\"257\" y=\"-1401.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (60, 30, 1)</text>\n</g>\n<!-- 140653847502128&#45;&gt;140653847500504 -->\n<g id=\"edge29\" class=\"edge\">\n<title>140653847502128&#45;&gt;140653847500504</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M257,-1393.9494C257,-1384.058 257,-1371.6435 257,-1361.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.5001,-1361.0288 257,-1351.0288 253.5001,-1361.0289 260.5001,-1361.0288\"/>\n</g>\n<!-- 140653847499440 -->\n<g id=\"node31\" class=\"node\">\n<title>140653847499440</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"391,-1358 337,-1358 337,-1323 391,-1323 391,-1358\"/>\n<text text-anchor=\"middle\" x=\"364\" y=\"-1330.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (60)</text>\n</g>\n<!-- 140653847499440&#45;&gt;140653850889240 -->\n<g id=\"edge30\" class=\"edge\">\n<title>140653847499440&#45;&gt;140653850889240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M336.9994,-1324.3501C320.6455,-1314.5683 299.872,-1302.1431 283.5924,-1292.4058\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"284.9726,-1289.153 274.594,-1287.0235 281.3793,-1295.1604 284.9726,-1289.153\"/>\n</g>\n<!-- 140653850889352 -->\n<g id=\"node32\" class=\"node\">\n<title>140653850889352</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"461,-1095 337,-1095 337,-1074 461,-1074 461,-1095\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-1081.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653850889352&#45;&gt;140653848071920 -->\n<g id=\"edge31\" class=\"edge\">\n<title>140653850889352&#45;&gt;140653848071920</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M399,-1073.9317C399,-1065.0913 399,-1052.2122 399,-1041.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"402.5001,-1041.2979 399,-1031.2979 395.5001,-1041.2979 402.5001,-1041.2979\"/>\n</g>\n<!-- 140653850887840 -->\n<g id=\"node33\" class=\"node\">\n<title>140653850887840</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"436.5,-1173 361.5,-1173 361.5,-1138 436.5,-1138 436.5,-1173\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-1145.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (120, 60, 1)</text>\n</g>\n<!-- 140653850887840&#45;&gt;140653850889352 -->\n<g id=\"edge32\" class=\"edge\">\n<title>140653850887840&#45;&gt;140653850889352</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M399,-1137.9494C399,-1128.058 399,-1115.6435 399,-1105.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"402.5001,-1105.0288 399,-1095.0288 395.5001,-1105.0289 402.5001,-1105.0288\"/>\n</g>\n<!-- 140653850889688 -->\n<g id=\"node34\" class=\"node\">\n<title>140653850889688</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"533,-1102 479,-1102 479,-1067 533,-1067 533,-1102\"/>\n<text text-anchor=\"middle\" x=\"506\" y=\"-1074.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (120)</text>\n</g>\n<!-- 140653850889688&#45;&gt;140653848071920 -->\n<g id=\"edge33\" class=\"edge\">\n<title>140653850889688&#45;&gt;140653848071920</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M478.9994,-1068.3501C462.6455,-1058.5683 441.872,-1046.1431 425.5924,-1036.4058\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"426.9726,-1033.153 416.594,-1031.0235 423.3793,-1039.1604 426.9726,-1033.153\"/>\n</g>\n<!-- 140653848071808 -->\n<g id=\"node35\" class=\"node\">\n<title>140653848071808</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"603,-839 479,-839 479,-818 603,-818 603,-839\"/>\n<text text-anchor=\"middle\" x=\"541\" y=\"-825.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653848071808&#45;&gt;140653847698008 -->\n<g id=\"edge34\" class=\"edge\">\n<title>140653848071808&#45;&gt;140653847698008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M541,-817.9317C541,-809.0913 541,-796.2122 541,-785.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"544.5001,-785.2979 541,-775.2979 537.5001,-785.2979 544.5001,-785.2979\"/>\n</g>\n<!-- 140653848070856 -->\n<g id=\"node36\" class=\"node\">\n<title>140653848070856</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"581.5,-917 500.5,-917 500.5,-882 581.5,-882 581.5,-917\"/>\n<text text-anchor=\"middle\" x=\"541\" y=\"-889.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (240, 120, 1)</text>\n</g>\n<!-- 140653848070856&#45;&gt;140653848071808 -->\n<g id=\"edge35\" class=\"edge\">\n<title>140653848070856&#45;&gt;140653848071808</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M541,-881.9494C541,-872.058 541,-859.6435 541,-849.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"544.5001,-849.0288 541,-839.0288 537.5001,-849.0289 544.5001,-849.0288\"/>\n</g>\n<!-- 140653848071976 -->\n<g id=\"node37\" class=\"node\">\n<title>140653848071976</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"675,-846 621,-846 621,-811 675,-811 675,-846\"/>\n<text text-anchor=\"middle\" x=\"648\" y=\"-818.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (240)</text>\n</g>\n<!-- 140653848071976&#45;&gt;140653847698008 -->\n<g id=\"edge36\" class=\"edge\">\n<title>140653848071976&#45;&gt;140653847698008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M620.9994,-812.3501C604.6455,-802.5683 583.872,-790.1431 567.5924,-780.4058\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"568.9726,-777.153 558.594,-775.0235 565.3793,-783.1604 568.9726,-777.153\"/>\n</g>\n<!-- 140653847828520 -->\n<g id=\"node38\" class=\"node\">\n<title>140653847828520</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"745,-647 621,-647 621,-626 745,-626 745,-647\"/>\n<text text-anchor=\"middle\" x=\"683\" y=\"-633.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653847828520&#45;&gt;140653847497696 -->\n<g id=\"edge37\" class=\"edge\">\n<title>140653847828520&#45;&gt;140653847497696</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M669.6454,-625.7787C659.431,-617.5784 645.1439,-606.1085 633.3489,-596.6393\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"635.2838,-593.7043 625.2947,-590.1732 630.9015,-599.1629 635.2838,-593.7043\"/>\n</g>\n<!-- 140653847698568 -->\n<g id=\"node39\" class=\"node\">\n<title>140653847698568</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"723.5,-718 642.5,-718 642.5,-683 723.5,-683 723.5,-718\"/>\n<text text-anchor=\"middle\" x=\"683\" y=\"-690.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (240, 240, 2)</text>\n</g>\n<!-- 140653847698568&#45;&gt;140653847828520 -->\n<g id=\"edge38\" class=\"edge\">\n<title>140653847698568&#45;&gt;140653847828520</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M683,-682.6724C683,-674.8405 683,-665.5893 683,-657.4323\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"686.5001,-657.2234 683,-647.2234 679.5001,-657.2235 686.5001,-657.2234\"/>\n</g>\n<!-- 140653847498480 -->\n<g id=\"node40\" class=\"node\">\n<title>140653847498480</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"816,-455 692,-455 692,-434 816,-434 816,-455\"/>\n<text text-anchor=\"middle\" x=\"754\" y=\"-441.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653847498480&#45;&gt;140653847498648 -->\n<g id=\"edge39\" class=\"edge\">\n<title>140653847498480&#45;&gt;140653847498648</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M754,-433.9317C754,-425.0913 754,-412.2122 754,-401.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"757.5001,-401.2979 754,-391.2979 750.5001,-401.2979 757.5001,-401.2979\"/>\n</g>\n<!-- 140653847497864 -->\n<g id=\"node41\" class=\"node\">\n<title>140653847497864</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"794.5,-533 713.5,-533 713.5,-498 794.5,-498 794.5,-533\"/>\n<text text-anchor=\"middle\" x=\"754\" y=\"-505.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (480, 240, 3)</text>\n</g>\n<!-- 140653847497864&#45;&gt;140653847498480 -->\n<g id=\"edge40\" class=\"edge\">\n<title>140653847497864&#45;&gt;140653847498480</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M754,-497.9494C754,-488.058 754,-475.6435 754,-465.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"757.5001,-465.0288 754,-455.0288 750.5001,-465.0289 757.5001,-465.0288\"/>\n</g>\n<!-- 140653847497584 -->\n<g id=\"node42\" class=\"node\">\n<title>140653847497584</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"888,-462 834,-462 834,-427 888,-427 888,-462\"/>\n<text text-anchor=\"middle\" x=\"861\" y=\"-434.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (480)</text>\n</g>\n<!-- 140653847497584&#45;&gt;140653847498648 -->\n<g id=\"edge41\" class=\"edge\">\n<title>140653847497584&#45;&gt;140653847498648</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M833.9994,-428.3501C817.6455,-418.5683 796.872,-406.1431 780.5924,-396.4058\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"781.9726,-393.153 771.594,-391.0235 778.3793,-399.1604 781.9726,-393.153\"/>\n</g>\n<!-- 140653847495232 -->\n<g id=\"node43\" class=\"node\">\n<title>140653847495232</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"958,-256 834,-256 834,-235 958,-235 958,-256\"/>\n<text text-anchor=\"middle\" x=\"896\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">UnsqueezeBackward0</text>\n</g>\n<!-- 140653847495232&#45;&gt;140653847495008 -->\n<g id=\"edge42\" class=\"edge\">\n<title>140653847495232&#45;&gt;140653847495008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M896,-234.9317C896,-226.0913 896,-213.2122 896,-202.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"899.5001,-202.2979 896,-192.2979 892.5001,-202.2979 899.5001,-202.2979\"/>\n</g>\n<!-- 140653847498592 -->\n<g id=\"node44\" class=\"node\">\n<title>140653847498592</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"930.5,-334 861.5,-334 861.5,-299 930.5,-299 930.5,-334\"/>\n<text text-anchor=\"middle\" x=\"896\" y=\"-306.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3, 480, 3)</text>\n</g>\n<!-- 140653847498592&#45;&gt;140653847495232 -->\n<g id=\"edge43\" class=\"edge\">\n<title>140653847498592&#45;&gt;140653847495232</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M896,-298.9494C896,-289.058 896,-276.6435 896,-266.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"899.5001,-266.0288 896,-256.0288 892.5001,-266.0289 899.5001,-266.0288\"/>\n</g>\n<!-- 140653847495288 -->\n<g id=\"node45\" class=\"node\">\n<title>140653847495288</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"1030,-263 976,-263 976,-228 1030,-228 1030,-263\"/>\n<text text-anchor=\"middle\" x=\"1003\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (3)</text>\n</g>\n<!-- 140653847495288&#45;&gt;140653847495008 -->\n<g id=\"edge44\" class=\"edge\">\n<title>140653847495288&#45;&gt;140653847495008</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M975.9994,-229.3501C959.6455,-219.5683 938.872,-207.1431 922.5924,-197.4058\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"923.9726,-194.153 913.594,-192.0235 920.3793,-200.1604 923.9726,-194.153\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VLy1RZTCUvl",
        "colab_type": "text"
      },
      "source": [
        "# Learning AE model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go_R0ONzCXi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training/test data pytorch tensors and associated  \n",
        "# list of tensors (xx[n][x] to access the nth sample for the xth field)\n",
        "training_dataset     = torch.utils.data.TensorDataset(torch.Tensor(x_train)) # create your datset\n",
        "test_dataset         = torch.utils.data.TensorDataset(torch.Tensor(x_test)) # create your datset\n",
        "\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True),\n",
        "    'val': torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True),\n",
        "}\n",
        "\n",
        "dataset_sizes = {'train': len(training_dataset), 'val': len(test_dataset)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY-xAJXQCe68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  use gpu if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# create a model from `AE` autoencoder class\n",
        "# load it to the specified device, either gpu or cpu\n",
        "model_AE  = model_AE.to(device)\n",
        "#model_AE.resnet = model_AE.decoder.resnet.to(device)\n",
        "\n",
        "# create an optimizer object\n",
        "# Adam optimizer with learning rate 1e-3\n",
        "optimizer        = optim.Adam(model_AE.parameters(), lr=1e-3)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# mean-squared error loss\n",
        "criterion = torch.nn.MSELoss()\n",
        "var_Tr    = np.var( x_train )\n",
        "var_Tt    = np.var( x_test )\n",
        "\n",
        "# training function\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_var  = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs_ in dataloaders[phase]:\n",
        "                inputs = inputs_[0]\n",
        "                inputs = inputs.to(device)\n",
        "                #print(inputs.size(0))\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    #loss = criterion(outputs, inputs)\n",
        "                    loss = torch.mean((outputs - inputs)**2)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss   += loss.item() * inputs.size(0)\n",
        "                #running_expvar += torch.sum( (outputs - inputs)**2 ) / torch.sum(\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss  = running_loss / dataset_sizes[phase]\n",
        "            if phase == 'train':\n",
        "              epoch_nloss = epoch_loss / var_Tr\n",
        "            else:\n",
        "              epoch_nloss = epoch_loss / var_Tt\n",
        "            #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} NLoss: {:.4f} '.format(\n",
        "                phase, epoch_loss, epoch_nloss))\n",
        "#            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "#                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYOndd-mCjSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "70b9578e-ab7a-4726-ba74-abd6cfc98330"
      },
      "source": [
        "# training AE model\n",
        "model_AE = train_model(model_AE, criterion, optimizer, exp_lr_scheduler,\n",
        "                       num_epochs=5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 0.0844 NLoss: 0.0845 \n",
            "val Loss: 0.0043 NLoss: 0.0043 \n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 0.0022 NLoss: 0.0022 \n",
            "val Loss: 0.0011 NLoss: 0.0011 \n",
            "\n",
            "Epoch 2/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-99f7e50b9394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_AE = train_model(model_AE, criterion, optimizer, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                        num_epochs=5)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-74f4e6923c47>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pgVLt29DuKF",
        "colab_type": "text"
      },
      "source": [
        "# Learning AE model from irregularly-sampled data (DinAE model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGa5BHlNDwpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model_AE_GradFP(torch.nn.Module):\n",
        "    def __init__(self,mod_AE,ShapeData,NiterProjection,NiterGrad,GradType,OptimType):\n",
        "    #def __init__(self,mod_AE,GradType,OptimType):\n",
        "        super(Model_AE_GradFP, self).__init__()\n",
        "        self.model_AE = mod_AE\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.GradType = GradType\n",
        "            self.OptimType = OptimType\n",
        "            self.NProjFP   = int(NiterProjection)\n",
        "            self.NGrad     = int(NiterGrad)\n",
        "            self.shape     = ShapeData\n",
        "        \n",
        "        if len(self.shape) == 2: ## 1D Data\n",
        "            if self.OptimType == 0: # fixed-step gradient descent\n",
        "              self.conv1    = torch.nn.Conv1d(self.shape[0], self.shape[0],1, padding=0)              \n",
        "            elif self.OptimType == 1: # ConvNet gradient descent using previous and current gradient\n",
        "              self.conv1    = torch.nn.Conv1d(2*self.shape[0], 8*self.shape[0],3, padding=1)\n",
        "              self.conv2    = torch.nn.Conv1d(8*self.shape[0], 16*self.shape[0],3, padding=1)\n",
        "              self.conv3    = torch.nn.Conv1d(16*self.shape[0], self.shape[0],3, padding=1)\n",
        "            elif self.OptimType == 2: # ConvNet gradient descent using previous and current gradient\n",
        "              self.lstm1    = ConvLSTM1d(self.shape[0],5*self.shape[0],3)\n",
        "              self.conv1    = torch.nn.Conv1d(5*self.shape[0], self.shape[0], 1, padding=0)           \n",
        "        elif len(self.shape) == 3: ## 2D Data            \n",
        "            if self.OptimType == 0: # fixed-step gradient descent\n",
        "              self.conv1    = torch.nn.Conv2d(self.shape[0], self.shape[0], (1,1), padding=0)\n",
        "            elif self.OptimType == 1: # ConvNet gradient descent using previous and current gradient\n",
        "              self.conv1    = torch.nn.Conv2d(2*self.shape[0], 8*self.shape[0], (3,3), padding=1)\n",
        "              self.conv2    = torch.nn.Conv2d(8*self.shape[0], 16*self.shape[0], (3,3), padding=1)\n",
        "              self.conv3    = torch.nn.Conv2d(16*self.shape[0], self.shape[0], (3,3), padding=1)\n",
        "            elif self.OptimType == 2: # ConvNet gradient descent using previous and current gradient\n",
        "              self.lstm1    = ConvLSTM2d(self.shape[0],5*self.shape[0],3)\n",
        "              self.conv1    = torch.nn.Conv2d(5*self.shape[0], self.shape[0], (1,1), padding=0)\n",
        "    def forward(self, x_inp,mask):\n",
        "        #mask   = torch.add(1.0,torch.mul(mask,0.0)) # set mask to 1 # debug\n",
        "        mask_  = torch.add(1.0,torch.mul(mask,-1.0)) #1. - mask\n",
        "        x      = torch.mul(x_inp,1.0)\n",
        "\n",
        "        # fixed-point iterations\n",
        "        if self.NProjFP > 0:\n",
        "          for kk in range(0,self.NProjFP):\n",
        "        #if NiterProjection > 0:\n",
        "        #  x      = torch.mul(x_inp,1.0)\n",
        "        #  for kk in range(0,NiterProjection):            \n",
        "            x_proj = self.model_AE(x)\n",
        "            x_proj = torch.mul(x_proj,mask_)\n",
        "            x      = torch.mul(x, mask)   \n",
        "            x      = torch.add(x , x_proj )\n",
        "\n",
        "        # gradient iteration\n",
        "        if self.NGrad > 0:\n",
        "          for kk in range(0,self.NGrad):\n",
        "        #if NiterGrad > 0:\n",
        "        #  for kk in range(0,NiterGrad):\n",
        "            # compute gradient\n",
        "            if self.GradType == 0: ## subgradient\n",
        "              grad = torch.add(self.model_AE(x),-1.,x)\n",
        "            else: ## true gradient using autograd\n",
        "              loss = torch.sum( torch.add(self.model_AE(x),-1.,x)**2 )\n",
        "              grad = torch.autograd.grad(loss,x)[0]\n",
        "            #grad = grad.view(-1,1,self.shape[1],self.shape[2])\n",
        "            grad.retain_grad()\n",
        "\n",
        "            # gradient step\n",
        "\n",
        "            if self.OptimType == 0: # fixed-step gradient\n",
        "              grad = self.conv1( grad )\n",
        "            elif self.OptimType == 1: # convNet for grad using previous and current gradient\n",
        "              if kk == 0:\n",
        "                grad_old = torch.randn(grad.size()).to(device)\n",
        "              gradAll  = torch.cat((grad_old,grad),1)\n",
        "              grad_old = torch.mul(1.,grad)\n",
        "\n",
        "              grad = self.conv1( gradAll )\n",
        "              grad = self.conv2( F.relu( grad ) )\n",
        "              grad = self.conv3( F.relu( grad ) )\n",
        "              #grad = grad.view(-1,self.shape[1],self.shape[2])\n",
        "\n",
        "            elif self.OptimType == 2: # convLSTLM suing grad as input\n",
        "              if kk == 0:\n",
        "                hidden,cell = self.lstm1(grad,None)\n",
        "              else:\n",
        "                hidden,cell = self.lstm1(grad,[hidden,cell])\n",
        "              grad = self.conv1(hidden)\n",
        "\n",
        "            # update\n",
        "            #grad  = grad.view(-1,self.shape[1],self.shape[2])\n",
        "            x_new = torch.add(x,grad)\n",
        "            x_new = torch.mul(x_new,mask_)\n",
        "            x     = torch.mul(x, mask)   \n",
        "            x     = torch.add(x , x_new )\n",
        "\n",
        "        x_proj        = self.model_AE(x)\n",
        "        return x_proj\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE6juhljD6Ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training function for dinAE\n",
        "def train_model(model, optimizer, scheduler,alpha, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    alpha_MaskedLoss = alpha[0]\n",
        "    alpha_GTLoss     = 1. - alpha[0]\n",
        "    alpha_AE         = alpha[1]\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                #rint('Learning')\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                #print('Evaluation')\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_loss_All     = 0.\n",
        "            running_loss_R       = 0.\n",
        "            running_loss_I       = 0.\n",
        "            running_loss_AE      = 0.\n",
        "            num_loss     = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            #for inputs_ in dataloaders[phase]:\n",
        "            #    inputs = inputs_[0].to(device)\n",
        "            for inputs_missing,masks,inputs_GT in dataloaders[phase]:\n",
        "                inputs_missing = inputs_missing.to(device)\n",
        "                masks          = masks.to(device)\n",
        "                inputs_GT      = inputs_GT.to(device)\n",
        "                #print(inputs.size(0))\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # need to evaluate grad/backward during the evaluation and training phase for model_AE\n",
        "                with torch.set_grad_enabled(True): \n",
        "                #with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs_missing,masks)\n",
        "                    #outputs = model(inputs)\n",
        "                    #loss = criterion( outputs,  inputs)\n",
        "                    loss_R      = torch.sum((outputs - inputs_GT)**2 * masks )\n",
        "                    loss_R      = torch.mul(1.0 / torch.sum(masks),loss_R)\n",
        "                    loss_I      = torch.sum((outputs - inputs_GT)**2 * (1. - masks) )\n",
        "                    loss_I      = torch.mul(1.0 / torch.sum(1.-masks),loss_I)\n",
        "                    loss_All    = torch.mean((outputs - inputs_GT)**2 )\n",
        "                    loss_AE     = torch.mean((model.model_AE(outputs) - outputs)**2 )\n",
        "                    loss_AE_GT  = torch.mean((model.model_AE(inputs_GT) - inputs_GT)**2 )\n",
        "                    \n",
        "                    if alpha_MaskedLoss > 0.:\n",
        "                        loss = torch.mul(alpha_MaskedLoss,loss_R)\n",
        "                    else: \n",
        "                        loss = torch.mul(alpha_GTLoss,loss_All)\n",
        "                    loss = torch.add(loss,torch.mul(alpha_AE,loss_AE))\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss             += loss.item() * inputs_missing.size(0)\n",
        "                running_loss_I           += loss_I.item() * inputs_missing.size(0)\n",
        "                running_loss_R           += loss_R.item() * inputs_missing.size(0)\n",
        "                running_loss_All         += loss_All.item() * inputs_missing.size(0)\n",
        "                running_loss_AE          += loss_AE_GT.item() * inputs_missing.size(0)\n",
        "                num_loss                 += inputs_missing.size(0)\n",
        "                #running_expvar += torch.sum( (outputs - inputs)**2 ) / torch.sum(\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss       = running_loss / num_loss\n",
        "            epoch_loss_All   = running_loss_All / num_loss\n",
        "            epoch_loss_AE    = running_loss_AE / num_loss\n",
        "            epoch_loss_I     = running_loss_I / num_loss\n",
        "            epoch_loss_R     = running_loss_R / num_loss\n",
        "            #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            if phase == 'train':\n",
        "              epoch_nloss_All = epoch_loss_All / var_Tr\n",
        "              epoch_nloss_I   = epoch_loss_I / var_Tr\n",
        "              epoch_nloss_R   = epoch_loss_R / var_Tr\n",
        "              epoch_nloss_AE  = loss_AE / var_Tr\n",
        "            else:\n",
        "              epoch_nloss_All = epoch_loss_All / var_Tt\n",
        "              epoch_nloss_I   = epoch_loss_I / var_Tt\n",
        "              epoch_nloss_R   = epoch_loss_R / var_Tt\n",
        "              epoch_nloss_AE   = loss_AE / var_Tt\n",
        "\n",
        "            #print('{} Loss: {:.4f} '.format(\n",
        "             #   phase, epoch_loss))\n",
        "            print('{} Loss: {:.4f} NLossAll: {:.4f} NLossR: {:.4f} NLossI: {:.4f} NLossAE: {:.4f}'.format(\n",
        "                phase, epoch_loss,epoch_nloss_All,epoch_nloss_R,epoch_nloss_I,epoch_nloss_AE))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgNrNN8zEAYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training/test data pytorch tensors and associated  \n",
        "# list of tensors (xx[n][x] to access the nth sample for the xth field)\n",
        "\n",
        "# no mask\n",
        "#training_dataset     = torch.utils.data.TensorDataset(torch.Tensor(x_train),torch.add(1.0,torch.mul(0.0,torch.Tensor(mask_train))),torch.Tensor(x_train)) # create your datset\n",
        "#test_dataset         = torch.utils.data.TensorDataset(torch.Tensor(x_test),torch.add(1.0,torch.mul(0.0,torch.Tensor(mask_test))),torch.Tensor(x_test)) # create your datset\n",
        "\n",
        "training_dataset     = torch.utils.data.TensorDataset(torch.Tensor(x_train_missing),torch.Tensor(mask_train),torch.Tensor(x_train)) # create your datset\n",
        "test_dataset         = torch.utils.data.TensorDataset(torch.Tensor(x_test_missing),torch.Tensor(mask_test),torch.Tensor(x_test)) # create your datset\n",
        "\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True),\n",
        "    'val': torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True),\n",
        "}\n",
        "\n",
        "dataset_sizes = {'train': len(training_dataset), 'val': len(test_dataset)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSN12SVGEBU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  use gpu if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# mean-squared error loss\n",
        "#criterion = torch.nn.MSELoss()\n",
        "var_Tr    = np.var( x_train )\n",
        "var_Tt    = np.var( x_test )\n",
        "\n",
        "if 1*1:\n",
        "  model_AE2    = Model_AE()\n",
        "\n",
        "if 1*1:\n",
        "    alpha           = np.array([1.0,0.1])\n",
        "    GradType        = 1 # Gradient computation (0: subgradient, 1: true gradient/autograd)\n",
        "    OptimType       = 1 # 0: fixed-step gradient descent, 1: ConvNet_step gradient descent, 2: LSTM-based descent\n",
        "    NiterProjection = 5 # Number of fixed-point iterations\n",
        "    NiterGrad       = 2 # Number of gradient descent step\n",
        "    \n",
        "    # NiterProjection,NiterGrad: global variables\n",
        "    # bug for NiterProjection = 0\n",
        "    shapeData       = x_train.shape[1:]\n",
        "    #model_AE_GradFP = Model_AE_GradFP(model_AE2,shapeData,NiterProjection,NiterGrad,GradType,OptimType)\n",
        "    model_AE_GradFP = dinAE.Model_AE_GradFP(model_AE2,shapeData,NiterProjection,NiterGrad,GradType,OptimType)\n",
        "\n",
        "    model_AE_GradFP = model_AE_GradFP.to(device)\n",
        "\n",
        "    # create an optimizer object\n",
        "    # Adam optimizer with learning rate 1e-3\n",
        "    optimizer        = optim.Adam(model_AE_GradFP.parameters(), lr=1e-4)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03UdsudIEMUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "081c2636-49de-4acc-9530-e4eb26e554d8"
      },
      "source": [
        "# model training\n",
        "model_AE_GradFP = train_model(model_AE_GradFP, optimizer, exp_lr_scheduler,\n",
        "                       alpha,num_epochs=100)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 0.0043 NLossAll: 0.0287 NLossR: 0.0041 NLossI: 0.0358 NLossAE: 0.0015\n",
            "val Loss: 0.0036 NLossAll: 0.0265 NLossR: 0.0034 NLossI: 0.0332 NLossAE: 0.0012\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 0.0033 NLossAll: 0.0259 NLossR: 0.0031 NLossI: 0.0325 NLossAE: 0.0012\n",
            "val Loss: 0.0029 NLossAll: 0.0244 NLossR: 0.0028 NLossI: 0.0307 NLossAE: 0.0010\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 0.0026 NLossAll: 0.0237 NLossR: 0.0025 NLossI: 0.0299 NLossAE: 0.0013\n",
            "val Loss: 0.0024 NLossAll: 0.0225 NLossR: 0.0023 NLossI: 0.0284 NLossAE: 0.0011\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 0.0022 NLossAll: 0.0220 NLossR: 0.0021 NLossI: 0.0278 NLossAE: 0.0010\n",
            "val Loss: 0.0021 NLossAll: 0.0206 NLossR: 0.0020 NLossI: 0.0260 NLossAE: 0.0009\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.0020 NLossAll: 0.0209 NLossR: 0.0019 NLossI: 0.0264 NLossAE: 0.0008\n",
            "val Loss: 0.0019 NLossAll: 0.0198 NLossR: 0.0018 NLossI: 0.0250 NLossAE: 0.0010\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.0019 NLossAll: 0.0202 NLossR: 0.0018 NLossI: 0.0255 NLossAE: 0.0010\n",
            "val Loss: 0.0022 NLossAll: 0.0208 NLossR: 0.0022 NLossI: 0.0262 NLossAE: 0.0009\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.0019 NLossAll: 0.0198 NLossR: 0.0018 NLossI: 0.0251 NLossAE: 0.0010\n",
            "val Loss: 0.0018 NLossAll: 0.0197 NLossR: 0.0018 NLossI: 0.0250 NLossAE: 0.0010\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.0017 NLossAll: 0.0195 NLossR: 0.0016 NLossI: 0.0246 NLossAE: 0.0009\n",
            "val Loss: 0.0017 NLossAll: 0.0189 NLossR: 0.0016 NLossI: 0.0239 NLossAE: 0.0009\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.0017 NLossAll: 0.0193 NLossR: 0.0016 NLossI: 0.0245 NLossAE: 0.0011\n",
            "val Loss: 0.0017 NLossAll: 0.0189 NLossR: 0.0016 NLossI: 0.0239 NLossAE: 0.0010\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.0017 NLossAll: 0.0193 NLossR: 0.0016 NLossI: 0.0244 NLossAE: 0.0010\n",
            "val Loss: 0.0017 NLossAll: 0.0190 NLossR: 0.0017 NLossI: 0.0240 NLossAE: 0.0009\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.0017 NLossAll: 0.0192 NLossR: 0.0016 NLossI: 0.0243 NLossAE: 0.0009\n",
            "val Loss: 0.0017 NLossAll: 0.0188 NLossR: 0.0016 NLossI: 0.0238 NLossAE: 0.0010\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.0017 NLossAll: 0.0192 NLossR: 0.0016 NLossI: 0.0242 NLossAE: 0.0009\n",
            "val Loss: 0.0017 NLossAll: 0.0188 NLossR: 0.0016 NLossI: 0.0238 NLossAE: 0.0009\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.0017 NLossAll: 0.0191 NLossR: 0.0016 NLossI: 0.0242 NLossAE: 0.0009\n",
            "val Loss: 0.0017 NLossAll: 0.0185 NLossR: 0.0016 NLossI: 0.0234 NLossAE: 0.0009\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.0017 NLossAll: 0.0190 NLossR: 0.0016 NLossI: 0.0241 NLossAE: 0.0010\n",
            "val Loss: 0.0017 NLossAll: 0.0185 NLossR: 0.0016 NLossI: 0.0234 NLossAE: 0.0011\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.0017 NLossAll: 0.0189 NLossR: 0.0016 NLossI: 0.0240 NLossAE: 0.0009\n",
            "val Loss: 0.0017 NLossAll: 0.0185 NLossR: 0.0016 NLossI: 0.0233 NLossAE: 0.0007\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.0017 NLossAll: 0.0189 NLossR: 0.0016 NLossI: 0.0239 NLossAE: 0.0008\n",
            "val Loss: 0.0017 NLossAll: 0.0183 NLossR: 0.0016 NLossI: 0.0231 NLossAE: 0.0009\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0188 NLossR: 0.0016 NLossI: 0.0238 NLossAE: 0.0011\n",
            "val Loss: 0.0017 NLossAll: 0.0184 NLossR: 0.0016 NLossI: 0.0233 NLossAE: 0.0008\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0187 NLossR: 0.0015 NLossI: 0.0237 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0016 NLossI: 0.0231 NLossAE: 0.0011\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0187 NLossR: 0.0015 NLossI: 0.0237 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0183 NLossR: 0.0015 NLossI: 0.0231 NLossAE: 0.0009\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0187 NLossR: 0.0015 NLossI: 0.0237 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0183 NLossR: 0.0015 NLossI: 0.0231 NLossAE: 0.0009\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0187 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0183 NLossR: 0.0015 NLossI: 0.0231 NLossAE: 0.0008\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0187 NLossR: 0.0015 NLossI: 0.0237 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0007\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0187 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0016 NLossI: 0.0231 NLossAE: 0.0010\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0231 NLossAE: 0.0010\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0229 NLossAE: 0.0010\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0010\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0011\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0007\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0007\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0011\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0010\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0010\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0010\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0010\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0007\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0010\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0007\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0010\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0010\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0007\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0007\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0011\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0010\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0181 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0235 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0010\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0008\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0009\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0011\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0009\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.0016 NLossAll: 0.0186 NLossR: 0.0015 NLossI: 0.0236 NLossAE: 0.0011\n",
            "val Loss: 0.0016 NLossAll: 0.0182 NLossR: 0.0015 NLossI: 0.0230 NLossAE: 0.0008\n",
            "\n",
            "Training complete in 45m 20s\n",
            "Best val loss: 0.001614\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}