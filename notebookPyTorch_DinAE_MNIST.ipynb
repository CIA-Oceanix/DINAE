{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebookPyTorch_DinAE_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOaJsvUA/mlr8TP/jJScIyv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rfablet/DinAE/blob/master/notebookPyTorch_DinAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhV5CxIkMpmw",
        "colab_type": "text"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l64w7gvOMXJ",
        "colab_type": "code",
        "outputId": "9a849dee-78a1-4f9f-d4b6-bb4b81bdd729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import os\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn import decomposition\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUmFTdFnVFDk",
        "colab_type": "code",
        "outputId": "88e7fe1f-ba83-49e5-bb38-6734f7fb9ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "!pip install torchviz\n",
        "import torchviz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.4.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3523 sha256=4a777b9d944e97e936fb33c85fa5842d404ee852110b63448ff50c20e291516a\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q35bMkwXVR-0",
        "colab_type": "code",
        "outputId": "e1c186f1-a5ad-4224-8519-a85bd71d7f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#!mkdir /content/PythonCode\n",
        "os.mkdir('/content/PythonCode')\n",
        "os.chdir('/content/PythonCode/')\n",
        "!git clone https://github.com/CIA-Oceanix/DinAE.git\n",
        "os.chdir('/content/PythonCode/DinAE')\n",
        "#!git pull\n",
        "#import DinAE"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DinAE'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/23)\u001b[K\rremote: Counting objects:   8% (2/23)\u001b[K\rremote: Counting objects:  13% (3/23)\u001b[K\rremote: Counting objects:  17% (4/23)\u001b[K\rremote: Counting objects:  21% (5/23)\u001b[K\rremote: Counting objects:  26% (6/23)\u001b[K\rremote: Counting objects:  30% (7/23)\u001b[K\rremote: Counting objects:  34% (8/23)\u001b[K\rremote: Counting objects:  39% (9/23)\u001b[K\rremote: Counting objects:  43% (10/23)\u001b[K\rremote: Counting objects:  47% (11/23)\u001b[K\rremote: Counting objects:  52% (12/23)\u001b[K\rremote: Counting objects:  56% (13/23)\u001b[K\rremote: Counting objects:  60% (14/23)\u001b[K\rremote: Counting objects:  65% (15/23)\u001b[K\rremote: Counting objects:  69% (16/23)\u001b[K\rremote: Counting objects:  73% (17/23)\u001b[K\rremote: Counting objects:  78% (18/23)\u001b[K\rremote: Counting objects:  82% (19/23)\u001b[K\rremote: Counting objects:  86% (20/23)\u001b[K\rremote: Counting objects:  91% (21/23)\u001b[K\rremote: Counting objects:  95% (22/23)\u001b[K\rremote: Counting objects: 100% (23/23)\u001b[K\rremote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects:   5% (1/17)\u001b[K\rremote: Compressing objects:  11% (2/17)\u001b[K\rremote: Compressing objects:  17% (3/17)\u001b[K\rremote: Compressing objects:  23% (4/17)\u001b[K\rremote: Compressing objects:  29% (5/17)\u001b[K\rremote: Compressing objects:  35% (6/17)\u001b[K\rremote: Compressing objects:  41% (7/17)\u001b[K\rremote: Compressing objects:  47% (8/17)\u001b[K\rremote: Compressing objects:  52% (9/17)\u001b[K\rremote: Compressing objects:  58% (10/17)\u001b[K\rremote: Compressing objects:  64% (11/17)\u001b[K\rremote: Compressing objects:  70% (12/17)\u001b[K\rremote: Compressing objects:  76% (13/17)\u001b[K\rremote: Compressing objects:  82% (14/17)\u001b[K\rremote: Compressing objects:  88% (15/17)\u001b[K\rremote: Compressing objects:  94% (16/17)\u001b[K\rremote: Compressing objects: 100% (17/17)\u001b[K\rremote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 23 (delta 6), reused 17 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects:   4% (1/23)   \rUnpacking objects:   8% (2/23)   \rUnpacking objects:  13% (3/23)   \rUnpacking objects:  17% (4/23)   \rUnpacking objects:  21% (5/23)   \rUnpacking objects:  26% (6/23)   \rUnpacking objects:  30% (7/23)   \rUnpacking objects:  34% (8/23)   \rUnpacking objects:  39% (9/23)   \rUnpacking objects:  43% (10/23)   \rUnpacking objects:  47% (11/23)   \rUnpacking objects:  52% (12/23)   \rUnpacking objects:  56% (13/23)   \rUnpacking objects:  60% (14/23)   \rUnpacking objects:  65% (15/23)   \rUnpacking objects:  69% (16/23)   \rUnpacking objects:  73% (17/23)   \rUnpacking objects:  78% (18/23)   \rUnpacking objects:  82% (19/23)   \rUnpacking objects:  86% (20/23)   \rUnpacking objects:  91% (21/23)   \rUnpacking objects:  95% (22/23)   \rUnpacking objects: 100% (23/23)   \rUnpacking objects: 100% (23/23), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z0kWSenXeya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.mkdir('/content/PythonCode')\n",
        "#os.chdir('/content/PythonCode')\n",
        "#!git clone https://github.com/CIA-Oceanix/DinAE.git\n",
        "os.chdir('/content/PythonCode/DinAE')\n",
        "import dinAE_solver_torch as dinAE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBoD1pUwM3bj",
        "colab_type": "text"
      },
      "source": [
        "# Load and prepare data (MNIST/image data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0OJYM8JOjVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ecc7e8df-1b64-4f7f-89e6-4d5861733490"
      },
      "source": [
        "### load datasets\n",
        "flagDataset = 0\n",
        "\n",
        "if flagDataset == 0: ## MNIST\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "elif flagDataset == 1: ## FASHION MNIST\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "#elif flagDataset == 2: ## OCCIPUT"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzV6xuhYOnDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data normalization\n",
        "if 1*1:\n",
        "  meanTr     = np.mean(x_train[:]) \n",
        "  x_train    = x_train - meanTr\n",
        "  x_test     = x_test - meanTr\n",
        "\n",
        "  # scale wrt std\n",
        "  stdTr      = np.sqrt( np.mean( x_train**2 ) )\n",
        "  x_train    = x_train / stdTr\n",
        "  x_test     = x_test / stdTr\n",
        "  \n",
        "else:\n",
        "  mini = np.amin(x_train[:])\n",
        "  maxi = np.amax(x_train[:])\n",
        "  \n",
        "  x_train = (x_train - mini ) /(maxi-mini)\n",
        "  x_test  = (x_test - mini ) /(maxi-mini)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUHDuTNuO-n2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Wsquare = int(4)\n",
        "Nsquare = int(3)\n",
        "\n",
        "# generate missing data areas for training data\n",
        "x_train_missing = np.copy(x_train).astype(float)\n",
        "mask_train      = np.zeros((x_train.shape))\n",
        "mask_test       = np.zeros((x_test.shape))\n",
        "\n",
        "for ii in range(x_train.shape[0]):\n",
        "  # generate mask\n",
        "  mask   = np.ones((x_train.shape[1],x_train.shape[2])).astype(float)\n",
        "  i_area = np.floor(np.random.uniform(Wsquare,x_train.shape[1]-Wsquare,Nsquare)).astype(int)\n",
        "  j_area = np.floor(np.random.uniform(Wsquare,x_train.shape[2]-Wsquare,Nsquare)).astype(int)\n",
        "  \n",
        "  for nn in range(Nsquare):\n",
        "    mask[i_area[nn]-Wsquare:i_area[nn]+Wsquare+1,j_area[nn]-Wsquare:j_area[nn]+Wsquare+1] = 0.\n",
        "    \n",
        "  # apply mask\n",
        "  x_train_missing[ii,:,:] *= mask\n",
        "  mask_train[ii,:,:]       = mask     \n",
        "  \n",
        "## generate missing data areas for test data\n",
        "x_test_missing = np.copy(x_test).astype(float)\n",
        "\n",
        "for ii in range(x_test.shape[0]):\n",
        "  # generate mask\n",
        "  mask   = np.ones((x_test.shape[1],x_test.shape[2])).astype(float)\n",
        "  i_area = np.floor(np.random.uniform(Wsquare,x_test.shape[1]-Wsquare,Nsquare)).astype(int)\n",
        "  j_area = np.floor(np.random.uniform(Wsquare,x_test.shape[2]-Wsquare,Nsquare)).astype(int)\n",
        "  \n",
        "  for nn in range(Nsquare):\n",
        "    mask[i_area[nn]-Wsquare:i_area[nn]+Wsquare+1,j_area[nn]-Wsquare:j_area[nn]+Wsquare+1] = 0.\n",
        "    \n",
        "  # apply mask\n",
        "  x_test_missing[ii,:,:] *= mask\n",
        "  mask_test[ii,:,:]       = mask     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on6EitLoPOtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random selection of indices for visualization\n",
        "indexes_train = np.random.permutation(x_train.shape[0])\n",
        "indexes_test  = np.random.permutation(x_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3BsJarQPdYF",
        "colab_type": "code",
        "outputId": "3c667c8d-7a26-4eeb-e39a-e6d9291f6fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "# visualize missing data pattern for training data\n",
        "plt.figure()\n",
        "for ii in range(5):\n",
        "    plt.subplot(2, 5, ii + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x_train[indexes_train[ii],:,:], cmap=plt.cm.gray_r)\n",
        "    plt.title('GT:%i' %(y_train[indexes_train[ii]]))\n",
        "    plt.subplot(2, 5, ii + 1+5)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x_train_missing[indexes_train[ii],:,:], cmap=plt.cm.gray_r)\n",
        "plt.show()\n",
        "\n",
        "# visualize missing data pattern for test data\n",
        "plt.figure()\n",
        "for ii in range(5):\n",
        "    plt.subplot(2, 5, ii + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x_test[indexes_test[ii],:,:], cmap=plt.cm.gray_r)\n",
        "    plt.title('GT:%i' %(y_test[indexes_test[ii]]))\n",
        "    plt.subplot(2, 5, ii + 1+5)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(x_test_missing[indexes_test[ii],:,:], cmap=plt.cm.gray_r)\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADOCAYAAACdDdHuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyJmDYyUimoMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADOCAYAAACdDdHuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EalHW7NPCui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For debugging: apply training to a subset of the datasets\n",
        "Ntr = 2000\n",
        "Ntt = 1000\n",
        "\n",
        "x_train = x_train[indexes_train[0:Ntr],:,:]\n",
        "y_train = y_train[indexes_train[0:Ntr]]\n",
        "x_train_missing = x_train_missing[indexes_train[0:Ntr],:,:]\n",
        "mask_train = mask_train[indexes_train[0:Ntr],:,:]\n",
        "\n",
        "x_test  = x_test[indexes_test[0:Ntt],:,:]\n",
        "y_test  = y_test[indexes_test[0:Ntt]]\n",
        "x_test_missing = x_test_missing[indexes_test[0:Ntt],:,:]\n",
        "mask_test = mask_test[indexes_test[0:Ntt],:,:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfj4X9p88js9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train         = x_train.reshape((x_train.shape[0],1,x_train.shape[1],x_train.shape[2]))\n",
        "x_train_missing = x_train_missing.reshape((x_train_missing.shape[0],1,x_train.shape[2],x_train.shape[3]))\n",
        "mask_train      = mask_train.reshape((x_train.shape[0],1,x_train.shape[2],x_train.shape[3]))\n",
        "\n",
        "x_test         = x_test.reshape((x_test.shape[0],1,x_test.shape[1],x_test.shape[2]))\n",
        "x_test_missing = x_test_missing.reshape((x_test.shape[0],1,x_test.shape[2],x_test.shape[3]))\n",
        "mask_test      = mask_test.reshape((x_test.shape[0],1,x_test.shape[2],x_test.shape[3]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz9jt7vhdSNg",
        "colab_type": "text"
      },
      "source": [
        "# PCA Decomposition & AE artchitecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBmY_dJodiTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DimAE      = 20#50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcxDm4jgPwUu",
        "colab_type": "code",
        "outputId": "178df3db-9d7e-4a8f-87f0-b22e7a7b6572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# PCA decomposition\n",
        "pca              = decomposition.PCA(DimAE)\n",
        "pca.fit(np.reshape(x_train,(x_train.shape[0],x_train.shape[1]*x_train.shape[2]*x_train.shape[3])))\n",
        "\n",
        "rec_PCA_Tt       = pca.transform(np.reshape(x_test,(x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3])))\n",
        "rec_PCA_Tt[:,DimAE:] = 0.\n",
        "rec_PCA_Tt       = pca.inverse_transform(rec_PCA_Tt)\n",
        "mse_PCA_Tt       = np.mean( (rec_PCA_Tt - x_test.reshape((x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3])))**2 )\n",
        "var_Tt           = np.mean( (x_test-np.mean(x_train,axis=0))** 2 )\n",
        "exp_var_PCA_Tt   = 1. - mse_PCA_Tt / var_Tt\n",
        "\n",
        "print(\".......... PCA Dim = %d\"%(DimAE))\n",
        "print('.... explained variance PCA (Tr) : %.2f%%'%(100.*np.cumsum(pca.explained_variance_ratio_)[DimAE-1]))\n",
        "print('.... explained variance PCA (Tt) : %.2f%%'%(100.*exp_var_PCA_Tt))\n",
        "\n",
        "# visualize PCs and associated projection\n",
        "PC              = np.zeros((DimAE+1,x_test.shape[1]*x_test.shape[2]*x_test.shape[3])) * float('NaN')                        \n",
        "PC[1:DimAE+1,:] = pca.components_\n",
        "PC[0,:]         = pca.mean_\n",
        "PC              = np.reshape(PC,(DimAE+1,x_test.shape[1],x_test.shape[2],x_test.shape[3]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".......... PCA Dim = 20\n",
            ".... explained variance PCA (Tr) : 64.85%\n",
            ".... explained variance PCA (Tt) : 64.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0kw3EDW5tFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNetConv2D(torch.nn.Module):\n",
        "  def __init__(self,Nblocks,dim,K,\n",
        "                 kernel_size,\n",
        "                 padding=0):\n",
        "      super(ResNetConv2D, self).__init__()\n",
        "      self.resnet = self._make_ResNet(Nblocks,dim,K,kernel_size,padding)\n",
        "\n",
        "  def _make_ResNet(self,Nblocks,dim,K,kernel_size,padding):\n",
        "      layers = []\n",
        "      for kk in range(0,Nblocks):\n",
        "        layers.append(torch.nn.Conv2d(dim,K*dim,kernel_size,padding=padding,bias=False))\n",
        "        layers.append(torch.nn.Conv2d(K*dim,dim,kernel_size,padding=padding,bias=False))\n",
        "\n",
        "      return torch.nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.resnet ( x )\n",
        "\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCbFN667BMRI",
        "colab_type": "code",
        "outputId": "39d395c3-10d6-4c2a-d71d-28a0249fa316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "resnet = ResNetConv2D(2,1,5,3,1)\n",
        "print(resnet)\n",
        "print('Number of trainable parameters = %d'%(sum(p.numel() for p in model_AE.parameters() if p.requires_grad)))\n",
        "\n",
        "#Model visualisation\n",
        "inputs = torch.randn(21,1,28,28)\n",
        "y = resnet(torch.autograd.Variable(inputs))\n",
        "print(y.size())\n",
        "torchviz.make_dot(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNetConv2D(\n",
            "  (resnet): Sequential(\n",
            "    (0): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (2): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (3): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  )\n",
            ")\n",
            "Number of trainable parameters = 248380\n",
            "torch.Size([21, 1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f6f0e7297b8>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"392pt\" height=\"313pt\"\n viewBox=\"0.00 0.00 391.50 313.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 309)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-309 387.5,-309 387.5,4 -4,4\"/>\n<!-- 140114960490448 -->\n<g id=\"node1\" class=\"node\">\n<title>140114960490448</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"363,-21 201,-21 201,0 363,0 363,-21\"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140114960487704 -->\n<g id=\"node2\" class=\"node\">\n<title>140114960487704</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"296,-85 134,-85 134,-64 296,-64 296,-85\"/>\n<text text-anchor=\"middle\" x=\"215\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140114960487704&#45;&gt;140114960490448 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140114960487704&#45;&gt;140114960490448</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M226.0637,-63.9317C236.1861,-54.2625 251.3665,-39.7619 263.3599,-28.3054\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"265.8824,-30.7362 270.6959,-21.2979 261.0472,-25.6744 265.8824,-30.7362\"/>\n</g>\n<!-- 140115255795272 -->\n<g id=\"node3\" class=\"node\">\n<title>140115255795272</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"229,-156 67,-156 67,-135 229,-135 229,-156\"/>\n<text text-anchor=\"middle\" x=\"148\" y=\"-142.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140115255795272&#45;&gt;140114960487704 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140115255795272&#45;&gt;140114960487704</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M158.1759,-134.7166C168.6044,-123.6655 185.0196,-106.2703 197.5311,-93.0118\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.4107,-95.06 204.7284,-85.3849 195.3196,-90.2557 200.4107,-95.06\"/>\n</g>\n<!-- 140115255795664 -->\n<g id=\"node4\" class=\"node\">\n<title>140115255795664</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"162,-227 0,-227 0,-206 162,-206 162,-227\"/>\n<text text-anchor=\"middle\" x=\"81\" y=\"-213.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 140115255795664&#45;&gt;140115255795272 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140115255795664&#45;&gt;140115255795272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M91.1759,-205.7166C101.6044,-194.6655 118.0196,-177.2703 130.5311,-164.0118\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"133.4107,-166.06 137.7284,-156.3849 128.3196,-161.2557 133.4107,-166.06\"/>\n</g>\n<!-- 140115255820472 -->\n<g id=\"node5\" class=\"node\">\n<title>140115255820472</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"115.5,-305 46.5,-305 46.5,-270 115.5,-270 115.5,-305\"/>\n<text text-anchor=\"middle\" x=\"81\" y=\"-277.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (5, 1, 3, 3)</text>\n</g>\n<!-- 140115255820472&#45;&gt;140115255795664 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140115255820472&#45;&gt;140115255795664</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M81,-269.9494C81,-260.058 81,-247.6435 81,-237.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"84.5001,-237.0288 81,-227.0288 77.5001,-237.0289 84.5001,-237.0288\"/>\n</g>\n<!-- 140115255820360 -->\n<g id=\"node6\" class=\"node\">\n<title>140115255820360</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"249.5,-234 180.5,-234 180.5,-199 249.5,-199 249.5,-234\"/>\n<text text-anchor=\"middle\" x=\"215\" y=\"-206.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1, 5, 3, 3)</text>\n</g>\n<!-- 140115255820360&#45;&gt;140115255795272 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140115255820360&#45;&gt;140115255795272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M198.4382,-198.9494C188.2103,-188.1109 175.1238,-174.2431 164.844,-163.3496\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"167.3445,-160.8997 157.9357,-156.0288 162.2534,-165.704 167.3445,-160.8997\"/>\n</g>\n<!-- 140115255795552 -->\n<g id=\"node7\" class=\"node\">\n<title>140115255795552</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"316.5,-163 247.5,-163 247.5,-128 316.5,-128 316.5,-163\"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (5, 1, 3, 3)</text>\n</g>\n<!-- 140115255795552&#45;&gt;140114960487704 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140115255795552&#45;&gt;140114960487704</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M265.4382,-127.9494C255.2103,-117.1109 242.1238,-103.2431 231.844,-92.3496\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"234.3445,-89.8997 224.9357,-85.0288 229.2534,-94.704 234.3445,-89.8997\"/>\n</g>\n<!-- 140115255794936 -->\n<g id=\"node8\" class=\"node\">\n<title>140115255794936</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"383.5,-92 314.5,-92 314.5,-57 383.5,-57 383.5,-92\"/>\n<text text-anchor=\"middle\" x=\"349\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1, 5, 3, 3)</text>\n</g>\n<!-- 140115255794936&#45;&gt;140114960490448 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140115255794936&#45;&gt;140114960490448</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M330.3368,-56.6724C321.0197,-47.7726 309.7839,-37.0398 300.4907,-28.1628\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"302.8748,-25.5999 293.2261,-21.2234 298.0396,-30.6617 302.8748,-25.5999\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lds7AJfYYavr",
        "colab_type": "code",
        "outputId": "fd014806-a9f3-4b9a-928f-dcc5fae14da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "flagAEType = 2\n",
        "DimAE      = 10#50\n",
        "dropout = 0.05\n",
        "wl2     = 0\n",
        "shapeData = x_train.shape[1:]\n",
        "\n",
        "if flagAEType == 0: ## MLP-AE\n",
        "\n",
        "  class Encoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Encoder, self).__init__()\n",
        "          self.fc1 = torch.nn.Linear(shapeData[0]*shapeData[1]*shapeData[2],6*DimAE)\n",
        "          self.fc2 = torch.nn.Linear(6*DimAE,2*DimAE)\n",
        "          self.fc3 = torch.nn.Linear(2*DimAE,DimAE)\n",
        "\n",
        "      def forward(self, x):\n",
        "          #x = self.fc1( torch.nn.Flatten(x) )\n",
        "          x = self.fc1( x.view(-1,shapeData[0]*shapeData[1]*shapeData[2]) )\n",
        "          x = self.fc2( F.relu(x) )\n",
        "          x = self.fc3( F.relu(x) )\n",
        "          return x\n",
        "\n",
        "  encoder = Encoder()\n",
        "\n",
        "  class Decoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Decoder, self).__init__()\n",
        "          self.fc1 = torch.nn.Linear(DimAE,10*DimAE)\n",
        "          self.fc2 = torch.nn.Linear(10*DimAE,20*DimAE)\n",
        "          self.fc3 = torch.nn.Linear(20*DimAE,shapeData[0]*shapeData[1]*shapeData[2])\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.fc1( x )\n",
        "          x = self.fc2( F.relu(x) )\n",
        "          x = self.fc3( F.relu(x) )\n",
        "          x = x.view(-1,shapeData[0],shapeData[1],shapeData[2])\n",
        "          return x\n",
        "\n",
        "elif flagAEType == 1: ## Conv-AE\n",
        "  Wpool_i = np.floor(  (np.floor((shapeData[1]-2)/2)-2)/2 ).astype(int) \n",
        "  Wpool_j = np.floor(  (np.floor((shapeData[2]-2)/2)-2)/2 ).astype(int)\n",
        "\n",
        "  class Encoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Encoder, self).__init__()\n",
        "          self.conv1 = torch.nn.Conv2d(shapeData[0],DimAE,(3,3),padding=0)\n",
        "          self.pool1 = torch.nn.AvgPool2d((2,2))\n",
        "          self.conv2 = torch.nn.Conv2d(DimAE,2*DimAE,(3,3),padding=0)\n",
        "          self.pool2 = torch.nn.AvgPool2d((2,2))\n",
        "          self.conv3 = torch.nn.Conv2d(2*DimAE,4*DimAE,(Wpool_i,Wpool_j),padding=0)\n",
        "          self.conv4 = torch.nn.Conv2d(4*DimAE,DimAE,(1,1),padding=0)\n",
        "\n",
        "      def forward(self, x):\n",
        "          #x = self.fc1( torch.nn.Flatten(x) )\n",
        "          x = self.conv1( x )\n",
        "          x = self.pool1(x)\n",
        "          x = self.conv2( F.relu(x) )\n",
        "          x = self.pool2(x)\n",
        "          x = self.conv3( F.relu(x) )\n",
        "          x = self.conv4( F.relu(x) )\n",
        "          x = x.view(-1,DimAE)\n",
        "          return x\n",
        "\n",
        "  class Decoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Decoder, self).__init__()\n",
        "          #self.conv1Tr = torch.nn.ConvTranspose2d(DimAE,1,(x_train.shape[1],x_train.shape[2]),stride=(x_train.shape[1],x_train.shape[2]),bias=False)\n",
        "          self.conv1Tr = torch.nn.ConvTranspose2d(DimAE,DimAE,(int(shapeData[1]/2),int(shapeData[2]/2)),stride=(int(shapeData[1]/2),int(shapeData[2]/2)),bias=False)\n",
        "          self.conv11   = torch.nn.Conv2d(DimAE,DimAE,(3,3),padding=1)\n",
        "          self.conv12   = torch.nn.Conv2d(DimAE,DimAE,(3,3),padding=1)\n",
        "          self.conv2Tr = torch.nn.ConvTranspose2d(DimAE,DimAE,(2,2),stride=(2,2),bias=False)\n",
        "          #self.resnet  = self._make_ResNet(2,DimAE,5,3,1)\n",
        "          self.resnet = dinAE.ResNetConv2D(2,DimAE,5,3,1)\n",
        "          self.convF   = torch.nn.Conv2d(DimAE,1,(1,1),padding=0)\n",
        "      def _make_ResNet(self,Nblocks,dim,K,kernel_size, padding):\n",
        "          layers = []\n",
        "          for kk in range(0,Nblocks):\n",
        "            layers.append(torch.nn.Conv2d(dim,K*dim,kernel_size,padding=padding,bias=False))\n",
        "            layers.append(torch.nn.Conv2d(K*dim,dim,kernel_size,padding=padding,bias=False))\n",
        "\n",
        "          return torch.nn.Sequential(*layers)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = x.view(-1,DimAE,1,1)\n",
        "          x = self.conv1Tr( x )\n",
        "          x = torch.add(self.conv12( F.relu( self.conv11(x) ) ),x)\n",
        "          x = torch.add(self.conv12( F.relu( self.conv11(x) ) ),x)\n",
        "          x = self.conv2Tr( x )\n",
        "          x = self.resnet(x)\n",
        "          x = self.convF(x)\n",
        "\n",
        "          #x = torch.add(self.conv22( F.relu( self.conv21(x) ) ),x)\n",
        "          #x = torch.add(self.conv22( F.relu( self.conv21(x) ) ),x)\n",
        "          #x = self.conv3( x )\n",
        "          #x = x.view(-1,shapeData[0],shapeData[1],shapeData[2])\n",
        "          return x\n",
        "\n",
        "elif flagAEType == 2: ## Conv model with no use of the central point\n",
        "  class Encoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Encoder, self).__init__()\n",
        "          self.pool1 = torch.nn.AvgPool2d((2,2))\n",
        "          self.conv1 = dinAE.ConstrainedConv2d(shapeData[0],shapeData[0]*DimAE,(3,3),padding=1)\n",
        "          self.conv2 = torch.nn.Conv2d(shapeData[0]*DimAE,2*shapeData[0]*DimAE,(1,1),padding=0)\n",
        "          self.conv3 = torch.nn.Conv2d(2*shapeData[0]*DimAE,4*shapeData[0]*DimAE,(1,1),padding=0)\n",
        "          self.conv4 = torch.nn.Conv2d(4*shapeData[0]*DimAE,8*shapeData[0]*DimAE,(1,1),padding=0)\n",
        "          self.conv2Tr = torch.nn.ConvTranspose2d(8*shapeData[0]*DimAE,8*shapeData[0]*DimAE,(2,2),stride=(2,2),bias=False)          \n",
        "          self.conv5 = torch.nn.Conv2d(8*shapeData[0]*DimAE,16*shapeData[0]*DimAE,(3,3),padding=1)\n",
        "          self.conv6 = torch.nn.Conv2d(16*shapeData[0]*DimAE,1,(3,3),padding=1)\n",
        "\n",
        "      def forward(self, x):\n",
        "          #x = self.fc1( torch.nn.Flatten(x) )\n",
        "          x = self.pool1( x )\n",
        "          x = self.conv1(x)\n",
        "          x = self.conv2( F.relu(x) )\n",
        "          x = self.conv3( F.relu(x) )\n",
        "          x = self.conv4( F.relu(x) )\n",
        "          x = self.conv2Tr( x )\n",
        "          x = self.conv5( x )\n",
        "          x = self.conv6( x )\n",
        "          x = x.view(-1,shapeData[0],shapeData[1],shapeData[2])\n",
        "          return x\n",
        "\n",
        "  class Decoder(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Decoder, self).__init__()\n",
        "\n",
        "      def forward(self, x):\n",
        "          return torch.mul(1.,x)\n",
        "\n",
        "class Model_AE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model_AE, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder( x )\n",
        "        x = self.decoder( x )\n",
        "        return x\n",
        "\n",
        "model_AE = Model_AE()\n",
        "\n",
        "print(model_AE)\n",
        "print('Number of trainable parameters = %d'%(sum(p.numel() for p in model_AE.parameters() if p.requires_grad)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model_AE(\n",
            "  (encoder): Encoder(\n",
            "    (pool1): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
            "    (conv1): ConstrainedConv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(10, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (conv3): Conv2d(20, 40, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (conv4): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (conv2Tr): ConvTranspose2d(80, 80, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
            "    (conv5): Conv2d(80, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv6): Conv2d(160, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (decoder): Decoder()\n",
            ")\n",
            "Number of trainable parameters = 146841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSHRMru9Ypwb",
        "colab_type": "code",
        "outputId": "04fbc56f-f9f8-4f80-e8dc-83aa40becf5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Model visualisation\n",
        "inputs = torch.randn(21,1,28,28)\n",
        "y = model_AE(torch.autograd.Variable(inputs))\n",
        "print(y.size())\n",
        "torchviz.make_dot(y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([21, 1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f4de1d72ac8>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"864pt\" height=\"795pt\"\n viewBox=\"0.00 0.00 864.00 795.35\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.9807 .9807) rotate(0) translate(4 807)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-807 877,-807 877,4 -4,4\"/>\n<!-- 139972458963968 -->\n<g id=\"node1\" class=\"node\">\n<title>139972458963968</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"805.5,-21 714.5,-21 714.5,0 805.5,0 805.5,-21\"/>\n<text text-anchor=\"middle\" x=\"760\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 139972458966936 -->\n<g id=\"node2\" class=\"node\">\n<title>139972458966936</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"805.5,-78 714.5,-78 714.5,-57 805.5,-57 805.5,-78\"/>\n<text text-anchor=\"middle\" x=\"760\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ViewBackward</text>\n</g>\n<!-- 139972458966936&#45;&gt;139972458963968 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139972458966936&#45;&gt;139972458963968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M760,-56.7787C760,-49.6134 760,-39.9517 760,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"763.5001,-31.1732 760,-21.1732 756.5001,-31.1732 763.5001,-31.1732\"/>\n</g>\n<!-- 139972458964584 -->\n<g id=\"node3\" class=\"node\">\n<title>139972458964584</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"841,-135 679,-135 679,-114 841,-114 841,-135\"/>\n<text text-anchor=\"middle\" x=\"760\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 139972458964584&#45;&gt;139972458966936 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139972458964584&#45;&gt;139972458966936</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M760,-113.7787C760,-106.6134 760,-96.9517 760,-88.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"763.5001,-88.1732 760,-78.1732 756.5001,-88.1732 763.5001,-88.1732\"/>\n</g>\n<!-- 139972458966712 -->\n<g id=\"node4\" class=\"node\">\n<title>139972458966712</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"701,-199 539,-199 539,-178 701,-178 701,-199\"/>\n<text text-anchor=\"middle\" x=\"620\" y=\"-185.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 139972458966712&#45;&gt;139972458964584 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139972458966712&#45;&gt;139972458964584</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M643.1182,-177.9317C666.1194,-167.4168 701.6193,-151.1883 727.462,-139.3745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"729.2079,-142.4248 736.8475,-135.084 726.2976,-136.0585 729.2079,-142.4248\"/>\n</g>\n<!-- 139972458965032 -->\n<g id=\"node5\" class=\"node\">\n<title>139972458965032</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"558.5,-270 379.5,-270 379.5,-249 558.5,-249 558.5,-270\"/>\n<text text-anchor=\"middle\" x=\"469\" y=\"-256.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SlowConvTranspose2DBackward</text>\n</g>\n<!-- 139972458965032&#45;&gt;139972458966712 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139972458965032&#45;&gt;139972458966712</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M491.6062,-248.8706C517.1817,-236.845 559.0305,-217.1678 587.9642,-203.5632\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"589.679,-206.6246 597.2392,-199.2021 586.7004,-200.2899 589.679,-206.6246\"/>\n</g>\n<!-- 139972458964528 -->\n<g id=\"node6\" class=\"node\">\n<title>139972458964528</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"480,-341 318,-341 318,-320 480,-320 480,-341\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-327.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 139972458964528&#45;&gt;139972458965032 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139972458964528&#45;&gt;139972458965032</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M409.6315,-319.7166C420.6298,-308.5612 438.0013,-290.9415 451.1178,-277.6377\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"453.7401,-279.9632 458.2685,-270.3849 448.7553,-275.0486 453.7401,-279.9632\"/>\n</g>\n<!-- 139972458966152 -->\n<g id=\"node7\" class=\"node\">\n<title>139972458966152</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"340,-412 246,-412 246,-391 340,-391 340,-412\"/>\n<text text-anchor=\"middle\" x=\"293\" y=\"-398.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139972458966152&#45;&gt;139972458964528 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139972458966152&#45;&gt;139972458964528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M309.0991,-390.7166C326.555,-379.0245 354.6129,-360.231 374.7416,-346.7486\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"376.736,-349.6253 383.0967,-341.1522 372.8405,-343.8094 376.736,-349.6253\"/>\n</g>\n<!-- 139972458965816 -->\n<g id=\"node8\" class=\"node\">\n<title>139972458965816</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"374,-476 212,-476 212,-455 374,-455 374,-476\"/>\n<text text-anchor=\"middle\" x=\"293\" y=\"-462.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 139972458965816&#45;&gt;139972458966152 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139972458965816&#45;&gt;139972458966152</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M293,-454.9317C293,-446.0913 293,-433.2122 293,-422.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"296.5001,-422.2979 293,-412.2979 289.5001,-422.2979 296.5001,-422.2979\"/>\n</g>\n<!-- 139972458965760 -->\n<g id=\"node9\" class=\"node\">\n<title>139972458965760</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"234,-540 140,-540 140,-519 234,-519 234,-540\"/>\n<text text-anchor=\"middle\" x=\"187\" y=\"-526.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139972458965760&#45;&gt;139972458965816 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139972458965760&#45;&gt;139972458965816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M204.5037,-518.9317C221.4587,-508.6948 247.3832,-493.0422 266.7976,-481.3203\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"268.7187,-484.249 275.4703,-476.084 265.1005,-478.2565 268.7187,-484.249\"/>\n</g>\n<!-- 139972458965928 -->\n<g id=\"node10\" class=\"node\">\n<title>139972458965928</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"268,-604 106,-604 106,-583 268,-583 268,-604\"/>\n<text text-anchor=\"middle\" x=\"187\" y=\"-590.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 139972458965928&#45;&gt;139972458965760 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139972458965928&#45;&gt;139972458965760</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M187,-582.9317C187,-574.0913 187,-561.2122 187,-550.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"190.5001,-550.2979 187,-540.2979 183.5001,-550.2979 190.5001,-550.2979\"/>\n</g>\n<!-- 139972458964920 -->\n<g id=\"node11\" class=\"node\">\n<title>139972458964920</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"128,-668 34,-668 34,-647 128,-647 128,-668\"/>\n<text text-anchor=\"middle\" x=\"81\" y=\"-654.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139972458964920&#45;&gt;139972458965928 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139972458964920&#45;&gt;139972458965928</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M98.5037,-646.9317C115.4587,-636.6948 141.3832,-621.0422 160.7976,-609.3203\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"162.7187,-612.249 169.4703,-604.084 159.1005,-606.2565 162.7187,-612.249\"/>\n</g>\n<!-- 139972458965312 -->\n<g id=\"node12\" class=\"node\">\n<title>139972458965312</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"162,-732 0,-732 0,-711 162,-711 162,-732\"/>\n<text text-anchor=\"middle\" x=\"81\" y=\"-718.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MkldnnConvolutionBackward</text>\n</g>\n<!-- 139972458965312&#45;&gt;139972458964920 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139972458965312&#45;&gt;139972458964920</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M81,-710.9317C81,-702.0913 81,-689.2122 81,-678.3135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"84.5001,-678.2979 81,-668.2979 77.5001,-678.2979 84.5001,-678.2979\"/>\n</g>\n<!-- 139972458965424 -->\n<g id=\"node13\" class=\"node\">\n<title>139972458965424</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"77.5,-803 2.5,-803 2.5,-768 77.5,-768 77.5,-803\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-775.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10, 1, 3, 3)</text>\n</g>\n<!-- 139972458965424&#45;&gt;139972458965312 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139972458965424&#45;&gt;139972458965312</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M51.4208,-767.6724C56.7802,-759.3066 63.1771,-749.3212 68.6488,-740.7799\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"71.6831,-742.5318 74.1303,-732.2234 65.7889,-738.7558 71.6831,-742.5318\"/>\n</g>\n<!-- 139972458964416 -->\n<g id=\"node14\" class=\"node\">\n<title>139972458964416</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"150,-803 96,-803 96,-768 150,-768 150,-803\"/>\n<text text-anchor=\"middle\" x=\"123\" y=\"-775.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (10)</text>\n</g>\n<!-- 139972458964416&#45;&gt;139972458965312 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139972458964416&#45;&gt;139972458965312</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M111.3007,-767.6724C105.8105,-759.3066 99.2576,-749.3212 93.6524,-740.7799\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.45,-738.6636 88.0373,-732.2234 90.5977,-742.5042 96.45,-738.6636\"/>\n</g>\n<!-- 139972458965536 -->\n<g id=\"node15\" class=\"node\">\n<title>139972458965536</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"227.5,-675 146.5,-675 146.5,-640 227.5,-640 227.5,-675\"/>\n<text text-anchor=\"middle\" x=\"187\" y=\"-647.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (20, 10, 1, 1)</text>\n</g>\n<!-- 139972458965536&#45;&gt;139972458965928 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139972458965536&#45;&gt;139972458965928</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M187,-639.6724C187,-631.8405 187,-622.5893 187,-614.4323\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"190.5001,-614.2234 187,-604.2234 183.5001,-614.2235 190.5001,-614.2234\"/>\n</g>\n<!-- 139972458965480 -->\n<g id=\"node16\" class=\"node\">\n<title>139972458965480</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"300,-675 246,-675 246,-640 300,-640 300,-675\"/>\n<text text-anchor=\"middle\" x=\"273\" y=\"-647.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (20)</text>\n</g>\n<!-- 139972458965480&#45;&gt;139972458965928 -->\n<g id=\"edge15\" class=\"edge\">\n<title>139972458965480&#45;&gt;139972458965928</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M249.0442,-639.6724C236.6462,-630.446 221.6013,-619.2498 209.4317,-610.1934\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"211.2694,-607.1981 201.1575,-604.0358 207.0903,-612.8138 211.2694,-607.1981\"/>\n</g>\n<!-- 139972458966096 -->\n<g id=\"node17\" class=\"node\">\n<title>139972458966096</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"333.5,-547 252.5,-547 252.5,-512 333.5,-512 333.5,-547\"/>\n<text text-anchor=\"middle\" x=\"293\" y=\"-519.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (40, 20, 1, 1)</text>\n</g>\n<!-- 139972458966096&#45;&gt;139972458965816 -->\n<g id=\"edge16\" class=\"edge\">\n<title>139972458966096&#45;&gt;139972458965816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M293,-511.6724C293,-503.8405 293,-494.5893 293,-486.4323\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"296.5001,-486.2234 293,-476.2234 289.5001,-486.2235 296.5001,-486.2234\"/>\n</g>\n<!-- 139972458966040 -->\n<g id=\"node18\" class=\"node\">\n<title>139972458966040</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"406,-547 352,-547 352,-512 406,-512 406,-547\"/>\n<text text-anchor=\"middle\" x=\"379\" y=\"-519.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (40)</text>\n</g>\n<!-- 139972458966040&#45;&gt;139972458965816 -->\n<g id=\"edge17\" class=\"edge\">\n<title>139972458966040&#45;&gt;139972458965816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M355.0442,-511.6724C342.6462,-502.446 327.6013,-491.2498 315.4317,-482.1934\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"317.2694,-479.1981 307.1575,-476.0358 313.0903,-484.8138 317.2694,-479.1981\"/>\n</g>\n<!-- 139972458965984 -->\n<g id=\"node19\" class=\"node\">\n<title>139972458965984</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"439.5,-419 358.5,-419 358.5,-384 439.5,-384 439.5,-419\"/>\n<text text-anchor=\"middle\" x=\"399\" y=\"-391.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (80, 40, 1, 1)</text>\n</g>\n<!-- 139972458965984&#45;&gt;139972458964528 -->\n<g id=\"edge18\" class=\"edge\">\n<title>139972458965984&#45;&gt;139972458964528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M399,-383.9494C399,-374.058 399,-361.6435 399,-351.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"402.5001,-351.0288 399,-341.0288 395.5001,-351.0289 402.5001,-351.0288\"/>\n</g>\n<!-- 139972458965872 -->\n<g id=\"node20\" class=\"node\">\n<title>139972458965872</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"512,-419 458,-419 458,-384 512,-384 512,-419\"/>\n<text text-anchor=\"middle\" x=\"485\" y=\"-391.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (80)</text>\n</g>\n<!-- 139972458965872&#45;&gt;139972458964528 -->\n<g id=\"edge19\" class=\"edge\">\n<title>139972458965872&#45;&gt;139972458964528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M463.7416,-383.9494C450.2308,-372.7952 432.834,-358.4327 419.477,-347.4054\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"421.6931,-344.6963 411.7532,-341.0288 417.2365,-350.0944 421.6931,-344.6963\"/>\n</g>\n<!-- 139972458964304 -->\n<g id=\"node21\" class=\"node\">\n<title>139972458964304</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"579.5,-348 498.5,-348 498.5,-313 579.5,-313 579.5,-348\"/>\n<text text-anchor=\"middle\" x=\"539\" y=\"-320.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (80, 80, 2, 2)</text>\n</g>\n<!-- 139972458964304&#45;&gt;139972458965032 -->\n<g id=\"edge20\" class=\"edge\">\n<title>139972458964304&#45;&gt;139972458965032</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M521.6966,-312.9494C511.0108,-302.1109 497.3383,-288.2431 486.5982,-277.3496\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"488.8937,-274.6926 479.3805,-270.0288 483.9089,-279.6072 488.8937,-274.6926\"/>\n</g>\n<!-- 139972458964640 -->\n<g id=\"node22\" class=\"node\">\n<title>139972458964640</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"663.5,-277 576.5,-277 576.5,-242 663.5,-242 663.5,-277\"/>\n<text text-anchor=\"middle\" x=\"620\" y=\"-249.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (160, 80, 3, 3)</text>\n</g>\n<!-- 139972458964640&#45;&gt;139972458966712 -->\n<g id=\"edge21\" class=\"edge\">\n<title>139972458964640&#45;&gt;139972458966712</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M620,-241.9494C620,-232.058 620,-219.6435 620,-209.2693\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"623.5001,-209.0288 620,-199.0288 616.5001,-209.0289 623.5001,-209.0288\"/>\n</g>\n<!-- 139972458964752 -->\n<g id=\"node23\" class=\"node\">\n<title>139972458964752</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"736,-277 682,-277 682,-242 736,-242 736,-277\"/>\n<text text-anchor=\"middle\" x=\"709\" y=\"-249.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (160)</text>\n</g>\n<!-- 139972458964752&#45;&gt;139972458966712 -->\n<g id=\"edge22\" class=\"edge\">\n<title>139972458964752&#45;&gt;139972458966712</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M687,-241.9494C673.018,-230.7952 655.0143,-216.4327 641.1913,-205.4054\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"643.1981,-202.5291 633.1981,-199.0288 638.8327,-208.0012 643.1981,-202.5291\"/>\n</g>\n<!-- 139972458964136 -->\n<g id=\"node24\" class=\"node\">\n<title>139972458964136</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"800.5,-206 719.5,-206 719.5,-171 800.5,-171 800.5,-206\"/>\n<text text-anchor=\"middle\" x=\"760\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1, 160, 3, 3)</text>\n</g>\n<!-- 139972458964136&#45;&gt;139972458964584 -->\n<g id=\"edge23\" class=\"edge\">\n<title>139972458964136&#45;&gt;139972458964584</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M760,-170.6724C760,-162.8405 760,-153.5893 760,-145.4323\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"763.5001,-145.2234 760,-135.2234 756.5001,-145.2235 763.5001,-145.2234\"/>\n</g>\n<!-- 139972458964248 -->\n<g id=\"node25\" class=\"node\">\n<title>139972458964248</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"873,-206 819,-206 819,-171 873,-171 873,-206\"/>\n<text text-anchor=\"middle\" x=\"846\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 139972458964248&#45;&gt;139972458964584 -->\n<g id=\"edge24\" class=\"edge\">\n<title>139972458964248&#45;&gt;139972458964584</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M822.0442,-170.6724C809.6462,-161.446 794.6013,-150.2498 782.4317,-141.1934\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"784.2694,-138.1981 774.1575,-135.0358 780.0903,-143.8138 784.2694,-138.1981\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI5jYHQwd2tp",
        "colab_type": "text"
      },
      "source": [
        "# Learning AE model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rq2cf5TZDRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training/test data pytorch tensors and associated  \n",
        "# list of tensors (xx[n][x] to access the nth sample for the xth field)\n",
        "training_dataset     = torch.utils.data.TensorDataset(torch.Tensor(x_train)) # create your datset\n",
        "test_dataset         = torch.utils.data.TensorDataset(torch.Tensor(x_test)) # create your datset\n",
        "\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True),\n",
        "    'val': torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True),\n",
        "}\n",
        "\n",
        "dataset_sizes = {'train': len(training_dataset), 'val': len(test_dataset)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVV21ui4ZIXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  use gpu if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# create a model from `AE` autoencoder class\n",
        "# load it to the specified device, either gpu or cpu\n",
        "model_AE  = model_AE.to(device)\n",
        "#model_AE.resnet = model_AE.decoder.resnet.to(device)\n",
        "\n",
        "# create an optimizer object\n",
        "# Adam optimizer with learning rate 1e-3\n",
        "optimizer        = optim.Adam(model_AE.parameters(), lr=1e-3)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# mean-squared error loss\n",
        "criterion = torch.nn.MSELoss()\n",
        "var_Tr    = np.var( x_train )\n",
        "var_Tt    = np.var( x_test )\n",
        "\n",
        "# training function\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_var  = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs_ in dataloaders[phase]:\n",
        "                inputs = inputs_[0]\n",
        "                inputs = inputs.to(device)\n",
        "                #print(inputs.size(0))\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    #loss = criterion(outputs, inputs)\n",
        "                    loss = torch.mean((outputs - inputs)**2)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss   += loss.item() * inputs.size(0)\n",
        "                #running_expvar += torch.sum( (outputs - inputs)**2 ) / torch.sum(\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss  = running_loss / dataset_sizes[phase]\n",
        "            if phase == 'train':\n",
        "              epoch_nloss = epoch_loss / var_Tr\n",
        "            else:\n",
        "              epoch_nloss = epoch_loss / var_Tt\n",
        "            #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} NLoss: {:.4f} '.format(\n",
        "                phase, epoch_loss, epoch_nloss))\n",
        "#            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "#                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf-la-loZOeQ",
        "colab_type": "code",
        "outputId": "9bcfe6de-9b01-486f-9047-f6ee507e78c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# training AE model\n",
        "model_AE = train_model(model_AE, criterion, optimizer, exp_lr_scheduler,\n",
        "                       num_epochs=10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "train Loss: 0.2753 NLoss: 0.2764 \n",
            "val Loss: 0.1046 NLoss: 0.1023 \n",
            "\n",
            "Training complete in 0m 28s\n",
            "Best val loss: 0.104618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuf9FiAYd9pJ",
        "colab_type": "text"
      },
      "source": [
        "# Learning AE model from irregularly-sampled data (DinAE model) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfozh6kaZSai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training function for dinAE\n",
        "def train_model(model, optimizer, scheduler,alpha, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    alpha_MaskedLoss = alpha[0]\n",
        "    alpha_GTLoss     = 1. - alpha[0]\n",
        "    alpha_AE         = alpha[1]\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                #rint('Learning')\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                #print('Evaluation')\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_loss_All     = 0.\n",
        "            running_loss_R       = 0.\n",
        "            running_loss_I       = 0.\n",
        "            running_loss_AE      = 0.\n",
        "            num_loss     = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            #for inputs_ in dataloaders[phase]:\n",
        "            #    inputs = inputs_[0].to(device)\n",
        "            for inputs_missing,masks,inputs_GT in dataloaders[phase]:\n",
        "                inputs_missing = inputs_missing.to(device)\n",
        "                masks          = masks.to(device)\n",
        "                inputs_GT      = inputs_GT.to(device)\n",
        "                #print(inputs.size(0))\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # need to evaluate grad/backward during the evaluation and training phase for model_AE\n",
        "                with torch.set_grad_enabled(True): \n",
        "                #with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs_missing,masks)\n",
        "                    #outputs = model(inputs)\n",
        "                    #loss = criterion( outputs,  inputs)\n",
        "                    loss_R      = torch.sum((outputs - inputs_GT)**2 * masks )\n",
        "                    loss_R      = torch.mul(1.0 / torch.sum(masks),loss_R)\n",
        "                    loss_I      = torch.sum((outputs - inputs_GT)**2 * (1. - masks) )\n",
        "                    loss_I      = torch.mul(1.0 / torch.sum(1.-masks),loss_I)\n",
        "                    loss_All    = torch.mean((outputs - inputs_GT)**2 )\n",
        "                    loss_AE     = torch.mean((model.model_AE(outputs) - outputs)**2 )\n",
        "                    loss_AE_GT  = torch.mean((model.model_AE(inputs_GT) - inputs_GT)**2 )\n",
        "                    \n",
        "                    if alpha_MaskedLoss > 0.:\n",
        "                        loss = torch.mul(alpha_MaskedLoss,loss_R)\n",
        "                    else: \n",
        "                        loss = torch.mul(alpha_GTLoss,loss_All)\n",
        "                    loss = torch.add(loss,torch.mul(alpha_AE,loss_AE))\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss             += loss.item() * inputs_missing.size(0)\n",
        "                running_loss_I           += loss_I.item() * inputs_missing.size(0)\n",
        "                running_loss_R           += loss_R.item() * inputs_missing.size(0)\n",
        "                running_loss_All         += loss_All.item() * inputs_missing.size(0)\n",
        "                running_loss_AE          += loss_AE_GT.item() * inputs_missing.size(0)\n",
        "                num_loss                 += inputs_missing.size(0)\n",
        "                #running_expvar += torch.sum( (outputs - inputs)**2 ) / torch.sum(\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss       = running_loss / num_loss\n",
        "            epoch_loss_All   = running_loss_All / num_loss\n",
        "            epoch_loss_AE    = running_loss_AE / num_loss\n",
        "            epoch_loss_I     = running_loss_I / num_loss\n",
        "            epoch_loss_R     = running_loss_R / num_loss\n",
        "            #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            if phase == 'train':\n",
        "              epoch_nloss_All = epoch_loss_All / var_Tr\n",
        "              epoch_nloss_I   = epoch_loss_I / var_Tr\n",
        "              epoch_nloss_R   = epoch_loss_R / var_Tr\n",
        "              epoch_nloss_AE  = loss_AE / var_Tr\n",
        "            else:\n",
        "              epoch_nloss_All = epoch_loss_All / var_Tt\n",
        "              epoch_nloss_I   = epoch_loss_I / var_Tt\n",
        "              epoch_nloss_R   = epoch_loss_R / var_Tt\n",
        "              epoch_nloss_AE   = loss_AE / var_Tt\n",
        "\n",
        "            #print('{} Loss: {:.4f} '.format(\n",
        "             #   phase, epoch_loss))\n",
        "            print('{} Loss: {:.4f} NLossAll: {:.4f} NLossR: {:.4f} NLossI: {:.4f} NLossAE: {:.4f}'.format(\n",
        "                phase, epoch_loss,epoch_nloss_All,epoch_nloss_R,epoch_nloss_I,epoch_nloss_AE))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVpMDT5dZnbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training/test data pytorch tensors and associated  \n",
        "# list of tensors (xx[n][x] to access the nth sample for the xth field)\n",
        "\n",
        "# no mask\n",
        "#training_dataset     = torch.utils.data.TensorDataset(torch.Tensor(x_train),torch.add(1.0,torch.mul(0.0,torch.Tensor(mask_train))),torch.Tensor(x_train)) # create your datset\n",
        "#test_dataset         = torch.utils.data.TensorDataset(torch.Tensor(x_test),torch.add(1.0,torch.mul(0.0,torch.Tensor(mask_test))),torch.Tensor(x_test)) # create your datset\n",
        "\n",
        "training_dataset     = torch.utils.data.TensorDataset(torch.Tensor(x_train_missing),torch.Tensor(mask_train),torch.Tensor(x_train)) # create your datset\n",
        "test_dataset         = torch.utils.data.TensorDataset(torch.Tensor(x_test_missing),torch.Tensor(mask_test),torch.Tensor(x_test)) # create your datset\n",
        "\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True),\n",
        "    'val': torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True),\n",
        "}\n",
        "\n",
        "dataset_sizes = {'train': len(training_dataset), 'val': len(test_dataset)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6pS0oGzZodA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  use gpu if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# mean-squared error loss\n",
        "#criterion = torch.nn.MSELoss()\n",
        "var_Tr    = np.var( x_train )\n",
        "var_Tt    = np.var( x_test )\n",
        "\n",
        "if 1*1:\n",
        "  model_AE2    = Model_AE()\n",
        "\n",
        "if 1*1:\n",
        "    alpha           = np.array([1.0,0.1])\n",
        "    GradType        = 1 # Gradient computation (0: subgradient, 1: true gradient/autograd)\n",
        "    OptimType       = 2 # 0: fixed-step gradient descent, 1: ConvNet_step gradient descent, 2: LSTM-based descent\n",
        "    NiterProjection = 2 # Number of fixed-point iterations\n",
        "    NiterGrad       = 2 # Number of gradient descent step\n",
        "    \n",
        "    # NiterProjection,NiterGrad: global variables\n",
        "    # bug for NiterProjection = 0\n",
        "    shapeData       = x_train.shape[1:]\n",
        "    #model_AE_GradFP = Model_AE_GradFP(model_AE2,shapeData,NiterProjection,NiterGrad,GradType,OptimType)\n",
        "    model_AE_GradFP = dinAE.Model_AE_GradFP(model_AE2,shapeData,NiterProjection,NiterGrad,GradType,OptimType)\n",
        "\n",
        "    model_AE_GradFP = model_AE_GradFP.to(device)\n",
        "\n",
        "    # create an optimizer object\n",
        "    # Adam optimizer with learning rate 1e-3\n",
        "    optimizer        = optim.Adam(model_AE_GradFP.parameters(), lr=1e-4)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otr2F2Wiuw8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "5d8c0c6f-5e92-4e98-d0ca-3480d49038f4"
      },
      "source": [
        "# model training\n",
        "model_AE_GradFP = train_model(model_AE_GradFP, optimizer, exp_lr_scheduler,\n",
        "                       alpha,num_epochs=1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "train Loss: 0.6349 NLossAll: 0.8662 NLossR: 0.6361 NLossI: 1.4968 NLossAE: 0.0653\n",
            "val Loss: 0.3967 NLossAll: 0.6332 NLossR: 0.3812 NLossI: 1.3241 NLossAE: 0.0675\n",
            "\n",
            "Training complete in 3m 17s\n",
            "Best val loss: 0.396671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ByMbcI1LdCr",
        "colab_type": "code",
        "outputId": "a4bfb917-6fec-42e4-d2b2-7c55a27f7369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model training\n",
        "model_AE_GradFP = train_model(model_AE_GradFP, optimizer, exp_lr_scheduler,alpha,num_epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 0.1009 NLossAll: 0.2558 NLossR: 0.0994 NLossI: 0.6856 NLossAE: 0.0122\n",
            "val Loss: 0.0997 NLossAll: 0.2502 NLossR: 0.0967 NLossI: 0.6713 NLossAE: 0.0174\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 0.1005 NLossAll: 0.2552 NLossR: 0.0990 NLossI: 0.6839 NLossAE: 0.0151\n",
            "val Loss: 0.0997 NLossAll: 0.2508 NLossR: 0.0966 NLossI: 0.6739 NLossAE: 0.0206\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 0.1002 NLossAll: 0.2546 NLossR: 0.0987 NLossI: 0.6829 NLossAE: 0.0138\n",
            "val Loss: 0.0997 NLossAll: 0.2514 NLossR: 0.0969 NLossI: 0.6755 NLossAE: 0.0131\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 0.1000 NLossAll: 0.2546 NLossR: 0.0984 NLossI: 0.6836 NLossAE: 0.0124\n",
            "val Loss: 0.0988 NLossAll: 0.2510 NLossR: 0.0959 NLossI: 0.6765 NLossAE: 0.0117\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.0996 NLossAll: 0.2537 NLossR: 0.0981 NLossI: 0.6810 NLossAE: 0.0140\n",
            "val Loss: 0.0984 NLossAll: 0.2496 NLossR: 0.0953 NLossI: 0.6730 NLossAE: 0.0172\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.0983 NLossAll: 0.2511 NLossR: 0.0969 NLossI: 0.6747 NLossAE: 0.0128\n",
            "val Loss: 0.0978 NLossAll: 0.2479 NLossR: 0.0950 NLossI: 0.6673 NLossAE: 0.0119\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.0983 NLossAll: 0.2513 NLossR: 0.0969 NLossI: 0.6753 NLossAE: 0.0161\n",
            "val Loss: 0.0980 NLossAll: 0.2477 NLossR: 0.0952 NLossI: 0.6662 NLossAE: 0.0117\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.0982 NLossAll: 0.2510 NLossR: 0.0969 NLossI: 0.6742 NLossAE: 0.0154\n",
            "val Loss: 0.0982 NLossAll: 0.2478 NLossR: 0.0953 NLossI: 0.6666 NLossAE: 0.0122\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.0981 NLossAll: 0.2508 NLossR: 0.0967 NLossI: 0.6739 NLossAE: 0.0112\n",
            "val Loss: 0.0979 NLossAll: 0.2479 NLossR: 0.0950 NLossI: 0.6674 NLossAE: 0.0147\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.0981 NLossAll: 0.2509 NLossR: 0.0967 NLossI: 0.6742 NLossAE: 0.0156\n",
            "val Loss: 0.0980 NLossAll: 0.2480 NLossR: 0.0952 NLossI: 0.6672 NLossAE: 0.0132\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.0980 NLossAll: 0.2509 NLossR: 0.0967 NLossI: 0.6746 NLossAE: 0.0121\n",
            "val Loss: 0.0977 NLossAll: 0.2479 NLossR: 0.0949 NLossI: 0.6679 NLossAE: 0.0120\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.0980 NLossAll: 0.2509 NLossR: 0.0966 NLossI: 0.6748 NLossAE: 0.0120\n",
            "val Loss: 0.0977 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6665 NLossAE: 0.0143\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.0980 NLossAll: 0.2509 NLossR: 0.0966 NLossI: 0.6746 NLossAE: 0.0120\n",
            "val Loss: 0.0976 NLossAll: 0.2477 NLossR: 0.0947 NLossI: 0.6673 NLossAE: 0.0138\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.0979 NLossAll: 0.2508 NLossR: 0.0966 NLossI: 0.6744 NLossAE: 0.0141\n",
            "val Loss: 0.0978 NLossAll: 0.2474 NLossR: 0.0949 NLossI: 0.6658 NLossAE: 0.0147\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.0979 NLossAll: 0.2508 NLossR: 0.0965 NLossI: 0.6745 NLossAE: 0.0129\n",
            "val Loss: 0.0978 NLossAll: 0.2477 NLossR: 0.0950 NLossI: 0.6667 NLossAE: 0.0137\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.0978 NLossAll: 0.2504 NLossR: 0.0965 NLossI: 0.6733 NLossAE: 0.0130\n",
            "val Loss: 0.0977 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6665 NLossAE: 0.0116\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.0978 NLossAll: 0.2505 NLossR: 0.0965 NLossI: 0.6738 NLossAE: 0.0114\n",
            "val Loss: 0.0977 NLossAll: 0.2475 NLossR: 0.0948 NLossI: 0.6662 NLossAE: 0.0119\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.0978 NLossAll: 0.2505 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0162\n",
            "val Loss: 0.0977 NLossAll: 0.2477 NLossR: 0.0949 NLossI: 0.6669 NLossAE: 0.0115\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.0978 NLossAll: 0.2505 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0122\n",
            "val Loss: 0.0976 NLossAll: 0.2477 NLossR: 0.0948 NLossI: 0.6673 NLossAE: 0.0137\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.0978 NLossAll: 0.2505 NLossR: 0.0964 NLossI: 0.6739 NLossAE: 0.0158\n",
            "val Loss: 0.0977 NLossAll: 0.2474 NLossR: 0.0948 NLossI: 0.6657 NLossAE: 0.0182\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6734 NLossAE: 0.0119\n",
            "val Loss: 0.0976 NLossAll: 0.2475 NLossR: 0.0948 NLossI: 0.6664 NLossAE: 0.0131\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0173\n",
            "val Loss: 0.0976 NLossAll: 0.2475 NLossR: 0.0948 NLossI: 0.6665 NLossAE: 0.0110\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6735 NLossAE: 0.0131\n",
            "val Loss: 0.0977 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0160\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2505 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0146\n",
            "val Loss: 0.0977 NLossAll: 0.2474 NLossR: 0.0948 NLossI: 0.6660 NLossAE: 0.0160\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0114\n",
            "val Loss: 0.0976 NLossAll: 0.2479 NLossR: 0.0948 NLossI: 0.6677 NLossAE: 0.0114\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2505 NLossR: 0.0963 NLossI: 0.6739 NLossAE: 0.0132\n",
            "val Loss: 0.0977 NLossAll: 0.2478 NLossR: 0.0948 NLossI: 0.6674 NLossAE: 0.0111\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2505 NLossR: 0.0964 NLossI: 0.6738 NLossAE: 0.0129\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6670 NLossAE: 0.0165\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0141\n",
            "val Loss: 0.0976 NLossAll: 0.2477 NLossR: 0.0948 NLossI: 0.6672 NLossAE: 0.0146\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0146\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6670 NLossAE: 0.0129\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0151\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0127\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0135\n",
            "val Loss: 0.0976 NLossAll: 0.2475 NLossR: 0.0948 NLossI: 0.6665 NLossAE: 0.0137\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6735 NLossAE: 0.0124\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0120\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6734 NLossAE: 0.0144\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0153\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6735 NLossAE: 0.0140\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6672 NLossAE: 0.0123\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0113\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6664 NLossAE: 0.0102\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0141\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0109\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2505 NLossR: 0.0964 NLossI: 0.6738 NLossAE: 0.0131\n",
            "val Loss: 0.0976 NLossAll: 0.2477 NLossR: 0.0948 NLossI: 0.6671 NLossAE: 0.0117\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2505 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0146\n",
            "val Loss: 0.0976 NLossAll: 0.2477 NLossR: 0.0948 NLossI: 0.6672 NLossAE: 0.0129\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2505 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0125\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0136\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0115\n",
            "val Loss: 0.0976 NLossAll: 0.2477 NLossR: 0.0948 NLossI: 0.6671 NLossAE: 0.0157\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6736 NLossAE: 0.0128\n",
            "val Loss: 0.0977 NLossAll: 0.2477 NLossR: 0.0948 NLossI: 0.6672 NLossAE: 0.0131\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6735 NLossAE: 0.0123\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6665 NLossAE: 0.0096\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0137\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0127\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6735 NLossAE: 0.0150\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6667 NLossAE: 0.0159\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6735 NLossAE: 0.0152\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0133\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0130\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6667 NLossAE: 0.0139\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0140\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6667 NLossAE: 0.0129\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6736 NLossAE: 0.0121\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0159\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0150\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6667 NLossAE: 0.0112\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0126\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0120\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0113\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6667 NLossAE: 0.0143\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0142\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6671 NLossAE: 0.0092\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0136\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6665 NLossAE: 0.0132\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0135\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6666 NLossAE: 0.0114\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0129\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0150\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0123\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0142\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0136\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0102\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0162\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0117\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0136\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6666 NLossAE: 0.0126\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0133\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0111\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2505 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0123\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6667 NLossAE: 0.0128\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0156\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0144\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6736 NLossAE: 0.0144\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0109\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0133\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0123\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0183\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6670 NLossAE: 0.0135\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6736 NLossAE: 0.0117\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6667 NLossAE: 0.0144\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0127\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6667 NLossAE: 0.0141\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6738 NLossAE: 0.0130\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0113\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0151\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6671 NLossAE: 0.0121\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0142\n",
            "val Loss: 0.0976 NLossAll: 0.2475 NLossR: 0.0948 NLossI: 0.6666 NLossAE: 0.0131\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6737 NLossAE: 0.0112\n",
            "val Loss: 0.0976 NLossAll: 0.2475 NLossR: 0.0948 NLossI: 0.6667 NLossAE: 0.0119\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6736 NLossAE: 0.0159\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0160\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6736 NLossAE: 0.0114\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0144\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6736 NLossAE: 0.0114\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0162\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6736 NLossAE: 0.0157\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0145\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0138\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0132\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6736 NLossAE: 0.0137\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0115\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0964 NLossI: 0.6736 NLossAE: 0.0162\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0141\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0158\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0137\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6738 NLossAE: 0.0171\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0123\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2504 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0137\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6668 NLossAE: 0.0115\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.0977 NLossAll: 0.2505 NLossR: 0.0963 NLossI: 0.6737 NLossAE: 0.0131\n",
            "val Loss: 0.0976 NLossAll: 0.2476 NLossR: 0.0948 NLossI: 0.6669 NLossAE: 0.0136\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVDi-qLfXhRa",
        "colab_type": "code",
        "outputId": "a6451a62-a195-47c9-e7e1-7253de04badd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "### Visualisation of results\n",
        "### Application model to test data\n",
        "\n",
        "# bug to be corrected \n",
        "def compute_loss(outputs,inputs_GT,outputs_AE,masks,shapeData):\n",
        "\n",
        "  # masked loss\n",
        "  diff = (outputs - inputs_GT)**2 * masks\n",
        "  masked_loss = torch.sum(diff.view(-1,shapeData[0]*shapeData[1]*shapeData[2]) , 1 )\n",
        "  masked_loss = masked_loss / torch.sum(masks.view(-1,shapeData[0]*shapeData[1]*shapeData[2]) , 1 )\n",
        "\n",
        "  # loss GT\n",
        "  diff = (outputs - inputs_GT)**2\n",
        "  loss_GT = torch.mean(diff.view(-1,shapeData[0]*shapeData[1]*shapeData[2]) , 1 )\n",
        "\n",
        "  # loss AE\n",
        "  diff = (outputs - outputs_AE)**2\n",
        "  loss_AE = torch.mean(diff.view(-1,shapeData[0]*shapeData[1]*shapeData[2]) , 1 )\n",
        "\n",
        "  return masked_loss,loss_GT,loss_AE\n",
        "\n",
        "# apply model\n",
        "outputs = []\n",
        "\n",
        "for inputs_missing,masks,inputs_GT in dataloaders['val']:\n",
        "  inputs_missing = inputs_missing.to(device)\n",
        "  masks          = masks.to(device)\n",
        "  inputs_GT      = inputs_GT.to(device)\n",
        "  with torch.set_grad_enabled(True): \n",
        "  #with torch.set_grad_enabled(phase == 'train'):\n",
        "      outputs_ = model_AE_GradFP(inputs_missing,masks)\n",
        "\n",
        "  # MSE\n",
        "  masked_loss_,loss_GT_,loss_AE_ = compute_loss(outputs_,inputs_GT,model_AE_GradFP.model_AE(outputs_),masks,shapeData)\n",
        "\n",
        "  if len(outputs) == 0:\n",
        "    outputs  = torch.mul(1.0,outputs_)\n",
        "    masked_loss = masked_loss_\n",
        "    loss_GT     = loss_GT_\n",
        "    loss_AE     = loss_AE_\n",
        "  else:\n",
        "    outputs     = torch.cat((outputs,outputs_),0)\n",
        "    masked_loss = torch.cat((masked_loss,masked_loss_),0)\n",
        "    loss_GT     = torch.cat((loss_GT,loss_GT_),0)\n",
        "    loss_AE     = torch.cat((loss_AE,loss_AE_),0)\n",
        "                    \n",
        "mean_masked_loss = torch.mean( masked_loss ).cpu().detach().numpy()\n",
        "mean_loss_GT     = torch.mean( loss_GT ).cpu().detach().numpy()\n",
        "mean_loss_AE     = torch.mean( loss_AE ).cpu().detach().numpy()\n",
        "\n",
        "v = np.var(x_test[:,:,:])\n",
        "\n",
        "# Visualisation\n",
        "# visualize missing data pattern for test data\n",
        "plt.figure()\n",
        "for ii in range(5):\n",
        "    plt.subplot(3, 5, ii + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inputs_GT[ii,:,:].cpu().detach().numpy().squeeze(), cmap=plt.cm.gray_r)\n",
        "    plt.title('GT')\n",
        "    plt.subplot(3, 5, ii + 1+5)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(inputs_missing[ii,:,:].cpu().detach().numpy().squeeze(), cmap=plt.cm.gray_r)\n",
        "    plt.title('Obs')\n",
        "    plt.subplot(3, 5, ii + 1+10)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(outputs[ii,:,:].cpu().detach().numpy().squeeze(), cmap=plt.cm.gray_r)\n",
        "    plt.title('Obs')\n",
        "plt.show()\n",
        "\n",
        "print('# Random test dataset: NLossMask: {:.4f} NLossGT: {:.4f} NLossAE: {:.4f}'.format(\n",
        "      mean_masked_loss/v, mean_loss_GT/v,mean_loss_AE/v))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0d497c576a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minputs_missing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs_GT\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0minputs_missing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_missing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mmasks\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
          ]
        }
      ]
    }
  ]
}